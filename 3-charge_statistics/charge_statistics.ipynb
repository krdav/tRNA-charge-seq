{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import os\n",
    "import sys\n",
    "from Bio import SeqIO, bgzf\n",
    "import gzip\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_folder = '/Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA_charging/tRNAseq_pilot/seq_data'\n",
    "sample_list = 'sample_list.xlsx'\n",
    "index_list = 'index_list.xlsx'\n",
    "os.chdir(seq_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "sample_fastq_dir = 'processed_fastq'\n",
    "umi_dir = 'UMI_trimmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read sample information ###\n",
    "sample_df = pd.read_excel(sample_list)\n",
    "index_df = pd.read_excel(index_list)\n",
    "\n",
    "# Read index sequences into dict:\n",
    "index_dict = dict()\n",
    "for t, i, s in zip(index_df['type'].values, index_df['id'].values, index_df['sequence'].values):\n",
    "    if t not in index_dict:\n",
    "        index_dict[t] = dict()\n",
    "    index_dict[t][i] = s\n",
    "\n",
    "# Add index sequences to dataframe:\n",
    "sample_df['P5_index_seq'] = [index_dict['P5_index'][i] for i in sample_df['P5_index'].values]\n",
    "sample_df['P7_index_seq'] = [index_dict['P7_index'][i] for i in sample_df['P7_index'].values]\n",
    "sample_df['barcode_seq'] = [index_dict['barcode'][i] for i in sample_df['barcode'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "### AdapterRomoval and paired end read merging ###\n",
    "\n",
    "adapter1_tmp = 'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC<P7_index>ATCTCGTATGCCGTCTTCTGCTTG'\n",
    "adapter2_tmp = 'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT<P5_index>GTGTAGATCTCGGTGGTCGCCGTATCATT'\n",
    "AR_cmd_tmp = [\"AdapterRemoval\", \"--gzip\", \"--preserve5p\", \"--collapse\", \"--minalignmentlength\", \"10\",]\n",
    "\n",
    "# Create folder for files:\n",
    "os.mkdir(AdapterRemoval_dir)\n",
    "os.chdir(AdapterRemoval_dir)\n",
    "\n",
    "# Generate list of files to merge:\n",
    "AR_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].drop_duplicates()\n",
    "\n",
    "# Merge files:\n",
    "N_pairs = list()\n",
    "N_merged = list()\n",
    "for index, row in AR_file_df.iterrows():\n",
    "    AR_cmd = AR_cmd_tmp.copy()\n",
    "    basename = '{}-{}'.format(row['P5_index'], row['P7_index'])\n",
    "    adapter1 = adapter1_tmp.replace('<P7_index>', row['P7_index_seq'])\n",
    "    adapter2 = adapter2_tmp.replace('<P5_index>', row['P5_index_seq'])\n",
    "\n",
    "    AR_cmd.extend(['--adapter1', adapter1])\n",
    "    AR_cmd.extend(['--adapter2', adapter2])\n",
    "    AR_cmd.extend(['--basename', basename])\n",
    "    AR_cmd.extend(['--file1', '../{}'.format(row['fastq_mate1_filename'])])\n",
    "    AR_cmd.extend(['--file2', '../{}'.format(row['fastq_mate2_filename'])])\n",
    "\n",
    "    with Popen(AR_cmd, stdout=PIPE, stderr=STDOUT, bufsize=1) as p, open('logfile.txt', 'a') as file:\n",
    "        file.write('Starting subprocess with command:')\n",
    "        file.write(str(AR_cmd))\n",
    "        file.write('\\n')\n",
    "        for line in p.stdout: # b'\\n'-separated lines\n",
    "            #sys.stdout.write(line) # pass bytes as is\n",
    "            file.write(line.decode('utf-8'))\n",
    "        file.write('\\n****** DONE ******\\n\\n\\n')\n",
    "\n",
    "    with open('{}.settings'.format(basename), 'r') as fh:\n",
    "        for line in fh:\n",
    "            if 'Total number of read pairs:' in line:\n",
    "                N_pairs.append(int(line.split(':')[1][1:]))\n",
    "            if 'Number of full-length collapsed pairs:' in line:\n",
    "                N_merged.append(int(line.split(':')[1][1:]))\n",
    "\n",
    "# Write stats:\n",
    "AR_file_df['N_pairs'] = N_pairs\n",
    "AR_file_df['N_merged'] = N_merged\n",
    "AR_file_df['percent_successfully_merged'] = AR_file_df['N_merged'].values / AR_file_df['N_pairs'].values *100\n",
    "AR_file_df.to_excel('merge_stats.xlsx')\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting into files based on barcode ###\n",
    "\n",
    "# Create folder for files:\n",
    "os.mkdir(sample_fastq_dir)\n",
    "os.chdir(sample_fastq_dir)\n",
    "\n",
    "\n",
    "# Map barcode sequences to reads:\n",
    "Nmapped = list()\n",
    "Ncc = {k:0 for k in sample_df['sample_name'].values}\n",
    "Ncca = {k:0 for k in sample_df['sample_name'].values}\n",
    "\n",
    "for index, row in AR_file_df.iterrows(): # Pull out each merged fastq file\n",
    "    basename = '{}-{}'.format(row['P5_index'], row['P7_index'])\n",
    "    merged_fastq_fn = '../{}/{}.collapsed.gz'.format(AdapterRemoval_dir, basename)\n",
    "    \n",
    "    # List the barcodes and associated sample names:\n",
    "    mask = (sample_df['P5_index'] == row['P5_index']) & (sample_df['P7_index'] == row['P7_index'])\n",
    "    bc_fh = [(k, v, gzip.open('{}.fastq.gz'.format(v), \"wt\")) for k, v in zip(sample_df[mask]['barcode_seq'].values, sample_df[mask]['sample_name'].values)]\n",
    "    \n",
    "    # Iterate over each record in the fastq file:\n",
    "    with gzip.open(merged_fastq_fn, \"rt\") as input_fh:\n",
    "        Nmapped.append(0)\n",
    "        for title, seq, qual in FastqGeneralIterator(input_fh):\n",
    "            # Search for barcodes and write to barcode specific file:\n",
    "            for bc, sample_name, fh in bc_fh:\n",
    "                bc_CC = 'CC' + bc\n",
    "                bc_CCA = 'CCA' + bc\n",
    "                if seq[-len(bc_CC):] == bc_CC:\n",
    "                    fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[:-len(bc)], qual[:-len(bc)]))\n",
    "                    Nmapped[-1] += 1\n",
    "                    Ncc[sample_name] += 1\n",
    "                elif seq[-len(bc_CCA):] == bc_CCA:\n",
    "                    fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[:-len(bc)], qual[:-len(bc)]))\n",
    "                    Nmapped[-1] += 1\n",
    "                    Ncca[sample_name] += 1\n",
    "    for bc, sample_name, fh in bc_fh:\n",
    "        fh.close()\n",
    "\n",
    "# Collect stats:\n",
    "AR_file_df['N_mapped'] = Nmapped\n",
    "AR_file_df['percent_mapped'] = AR_file_df['N_mapped'].values / AR_file_df['N_merged'].values *100\n",
    "\n",
    "sample_df['N_CC'] = [Ncc[sn] for sn in sample_df['sample_name']]\n",
    "sample_df['N_CCA'] = [Ncca[sn] for sn in sample_df['sample_name']]\n",
    "sample_df['N_seqs'] = sample_df['N_CC'].values + sample_df['N_CCA'].values\n",
    "sample_df['percent_charging'] = sample_df['N_CCA'].values / sample_df['N_seqs'].values *100\n",
    "\n",
    "AR_file_df.to_excel('index-pair_stats.xlsx')\n",
    "sample_df.to_excel('sample_stats.xlsx')\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate UMI stats and wrie final trimmed tRNA sequences ###\n",
    "# Note, the cDNA input amount is so large that it is very unlikely to sequence\n",
    "# the same PCR amplified DNA twice. Therefore, this processing step does not\n",
    "# attempt to merge possible UMI duplicates.\n",
    "\n",
    "# From: https://stats.stackexchange.com/questions/296005/the-expected-number-of-unique-elements-drawn-with-replacement\n",
    "# I get the expected number of unique UMIs:\n",
    "# E_X = n*(1-((n-1) / n)**k)\n",
    "# Where k = is the number of sequences (draws)\n",
    "# and n = to the number of possible UMIs (bins)\n",
    "n_bins = 4**9 * 2 # number of UMI bins (n)\n",
    "\n",
    "\n",
    "# Create folder for files:\n",
    "os.mkdir(umi_dir)\n",
    "os.chdir(umi_dir)\n",
    "\n",
    "# Trim UMIs off sequences:\n",
    "N_umi_obs = list()\n",
    "N_umi_exp = list()\n",
    "N_seq_list = list()\n",
    "for index, row in sample_df.iterrows(): # Process each sample individually\n",
    "    fastq_name = '../{}/{}.fastq.gz'.format(sample_fastq_dir, row['sample_name'])\n",
    "    UMIs = set()\n",
    "    Nseqs = 0\n",
    "    with gzip.open('{}_UMI-trimmed.fastq.gz'.format(row['sample_name']), \"wt\") as output_fh:\n",
    "        with gzip.open(fastq_name, \"rt\") as input_fh:\n",
    "            for title, seq, qual in FastqGeneralIterator(input_fh):\n",
    "                umi = seq[0:10]\n",
    "                if umi[-1] == 'T' or umi[-1] == 'C': # UMI sequence requirement\n",
    "                    UMIs.add(umi)\n",
    "                    Nseqs += 1\n",
    "                    # Write the trimmed sequence:\n",
    "                    output_fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[10:], qual[10:]))\n",
    "    # Calculate the observed and expected UMI count:\n",
    "    N_seq_list.append(Nseqs)\n",
    "    k_draws = Nseqs\n",
    "    N_umi_obs.append(len(UMIs))\n",
    "    E_X = n_bins*(1-((n_bins-1) / n_bins)**k_draws)\n",
    "    N_umi_exp.append(round(E_X))\n",
    "\n",
    "# Collect UMI stats:\n",
    "sample_df['N_UMI_observed'] = N_umi_obs\n",
    "sample_df['N_UMI_expected'] = N_umi_exp\n",
    "sample_df['percent_seqs_after_UMI_trim'] = np.array(N_seq_list) / sample_df['N_seqs'].values * 100\n",
    "sample_df['percent_UMI_obs-vs-exp'] = sample_df['N_UMI_observed'].values / sample_df['N_UMI_expected'].values * 100\n",
    "sample_df.to_excel('sample_UMI_stats.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
