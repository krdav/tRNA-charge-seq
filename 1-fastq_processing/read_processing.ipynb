{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq/1-fastq_processing\n"
     ]
    }
   ],
   "source": [
    "import os, sys, shutil, bz2\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO, bgzf\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "\n",
    "# Navigate back to workbookDir in case of re-running a code block:\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "os.chdir(workbookDir)  # If you changed the current working dir, this will take you back to the workbook dir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "gggg\n",
    "ssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These folder/files change depending on dataset:\n",
    "data_folder = 'data/pilot_exp'\n",
    "project_folder = 'projects/pilot_exp'\n",
    "seq_folder = 'raw_fastq'\n",
    "index_list = 'index_list.xlsx'\n",
    "sample_list = 'sample_list.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These folder names are used in subsequent steps, so do not change:\n",
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "sample_fastq_dir = 'processed_fastq'\n",
    "umi_dir = 'UMI_trimmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample/index information:\n",
    "index_df = pd.read_excel(index_list)\n",
    "sample_df = pd.read_excel('../' + project_folder + '/' + sample_list)\n",
    "\n",
    "# Make folder structure for data and stats:\n",
    "os.chdir('../' + data_folder)\n",
    "stats_dir = '../../' + project_folder + '/read_processing_stats'\n",
    "try:\n",
    "    os.mkdir(stats_dir)\n",
    "except:\n",
    "    shutil.rmtree(stats_dir)\n",
    "    os.mkdir(stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read index sequences into dict:\n",
    "index_dict = dict()\n",
    "for t, i, s in zip(index_df['type'].values, index_df['id'].values, index_df['sequence'].values):\n",
    "    if t not in index_dict:\n",
    "        index_dict[t] = dict()\n",
    "    index_dict[t][i] = s\n",
    "\n",
    "# Add index sequences to dataframe:\n",
    "sample_df['P5_index_seq'] = [index_dict['P5_index'][i] for i in sample_df['P5_index'].values]\n",
    "sample_df['P7_index_seq'] = [index_dict['P7_index'][i] for i in sample_df['P7_index'].values]\n",
    "sample_df['barcode_seq'] = [index_dict['barcode'][i] for i in sample_df['barcode'].values]\n",
    "\n",
    "# Add seq_folder to filename:\n",
    "sample_df['fastq_mate1_filename'] = [seq_folder + '/' + fn for fn in sample_df['fastq_mate1_filename'].values]\n",
    "sample_df['fastq_mate2_filename'] = [seq_folder + '/' + fn for fn in sample_df['fastq_mate2_filename'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../projects/pilot_exp/read_processing_stats/merge_stats.xlsx'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### AdapterRomoval and paired end read merging ###\n",
    "\n",
    "adapter1_tmp = 'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC<P7_index>ATCTCGTATGCCGTCTTCTGCTTG'\n",
    "adapter2_tmp = 'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT<P5_index>GTGTAGATCTCGGTGGTCGCCGTATCATT'\n",
    "AR_cmd_tmp = [\"AdapterRemoval\", \"--bzip2\", \"--preserve5p\", \"--collapse\", \"--minalignmentlength\", \"10\", \"--threads\", \"8\"]\n",
    "\n",
    "# Create folder for files:\n",
    "try:\n",
    "    os.mkdir(AdapterRemoval_dir)\n",
    "except:\n",
    "    shutil.rmtree(AdapterRemoval_dir)\n",
    "    os.mkdir(AdapterRemoval_dir)\n",
    "os.chdir(AdapterRemoval_dir)\n",
    "\n",
    "# Generate list of files to merge:\n",
    "AR_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].drop_duplicates()\n",
    "\n",
    "# Merge files:\n",
    "N_pairs = list()\n",
    "N_merged = list()\n",
    "for index, row in AR_file_df.iterrows():\n",
    "    AR_cmd = AR_cmd_tmp.copy()\n",
    "    basename = '{}-{}'.format(row['P5_index'], row['P7_index'])\n",
    "    adapter1 = adapter1_tmp.replace('<P7_index>', row['P7_index_seq'])\n",
    "    adapter2 = adapter2_tmp.replace('<P5_index>', row['P5_index_seq'])\n",
    "\n",
    "    AR_cmd.extend(['--adapter1', adapter1])\n",
    "    AR_cmd.extend(['--adapter2', adapter2])\n",
    "    AR_cmd.extend(['--basename', basename])\n",
    "    AR_cmd.extend(['--file1', '../{}'.format(row['fastq_mate1_filename'])])\n",
    "    AR_cmd.extend(['--file2', '../{}'.format(row['fastq_mate2_filename'])])\n",
    "\n",
    "    with Popen(AR_cmd, stdout=PIPE, stderr=STDOUT, bufsize=1) as p, open('logfile.txt', 'a') as file:\n",
    "        file.write('Starting subprocess with command:')\n",
    "        file.write(str(AR_cmd))\n",
    "        file.write('\\n')\n",
    "        for line in p.stdout: # b'\\n'-separated lines\n",
    "            #sys.stdout.write(line) # pass bytes as is\n",
    "            file.write(line.decode('utf-8'))\n",
    "        file.write('\\n****** DONE ******\\n\\n\\n')\n",
    "\n",
    "    with open('{}.settings'.format(basename), 'r') as fh:\n",
    "        for line in fh:\n",
    "            if 'Total number of read pairs:' in line:\n",
    "                N_pairs.append(int(line.split(':')[1][1:]))\n",
    "            if 'Number of full-length collapsed pairs:' in line:\n",
    "                N_merged.append(int(line.split(':')[1][1:]))\n",
    "\n",
    "# Write stats:\n",
    "AR_file_df['N_pairs'] = N_pairs\n",
    "AR_file_df['N_merged'] = N_merged\n",
    "AR_file_df['percent_successfully_merged'] = AR_file_df['N_merged'].values / AR_file_df['N_pairs'].values *100\n",
    "AR_file_df.to_excel('merge_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(AdapterRemoval_dir + '/merge_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../projects/pilot_exp/read_processing_stats/index-pair_stats.xlsx'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Splitting into files based on barcode ###\n",
    "\n",
    "# Create folder for files:\n",
    "try:\n",
    "    os.mkdir(sample_fastq_dir)\n",
    "except:\n",
    "    shutil.rmtree(sample_fastq_dir)\n",
    "    os.mkdir(sample_fastq_dir)\n",
    "os.chdir(sample_fastq_dir)\n",
    "\n",
    "\n",
    "# Map barcode sequences to reads:\n",
    "Nmapped = list()\n",
    "Ncc = {k:0 for k in sample_df['sample_name_unique'].values}\n",
    "Ncca = {k:0 for k in sample_df['sample_name_unique'].values}\n",
    "Ntot = {k:0 for k in sample_df['sample_name_unique'].values}\n",
    "\n",
    "for index, row in AR_file_df.iterrows(): # Pull out each merged fastq file\n",
    "    basename = '{}-{}'.format(row['P5_index'], row['P7_index'])\n",
    "    merged_fastq_fn = '../{}/{}.collapsed.bz2'.format(AdapterRemoval_dir, basename)\n",
    "    \n",
    "    # List the barcodes and associated sample names:\n",
    "    mask = (sample_df['P5_index'] == row['P5_index']) & (sample_df['P7_index'] == row['P7_index'])\n",
    "    bc_fh = [(k, v, bz2.open('{}.fastq.bz2'.format(v), \"wt\")) for k, v in zip(sample_df[mask]['barcode_seq'].values, sample_df[mask]['sample_name_unique'].values)]\n",
    "    \n",
    "    # Iterate over each record in the fastq file:\n",
    "    with bz2.open(merged_fastq_fn, \"rt\") as input_fh:\n",
    "        Nmapped.append(0)\n",
    "        for title, seq, qual in FastqGeneralIterator(input_fh):\n",
    "            # Search for barcodes and write to barcode specific file:\n",
    "            for bc, sample_name, fh in bc_fh:\n",
    "                if seq[-len(bc):] == bc:\n",
    "                    fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[:-len(bc)], qual[:-len(bc)]))\n",
    "                    Nmapped[-1] += 1\n",
    "                    Ntot[sample_name] += 1\n",
    "                    # Count if CC, CCA or not:\n",
    "                    bc_CC = 'CC' + bc\n",
    "                    bc_CCA = 'CCA' + bc\n",
    "                    if seq[-len(bc_CC):] == bc_CC:\n",
    "                        Ncc[sample_name] += 1\n",
    "                    elif seq[-len(bc_CCA):] == bc_CCA:\n",
    "                        Ncca[sample_name] += 1\n",
    "    for bc, sample_name, fh in bc_fh:\n",
    "        fh.close()\n",
    "\n",
    "# Collect stats:\n",
    "AR_file_df['N_mapped'] = Nmapped\n",
    "AR_file_df['percent_mapped'] = AR_file_df['N_mapped'].values / AR_file_df['N_merged'].values *100\n",
    "\n",
    "sample_df['N_total'] = [Ntot[sn] for sn in sample_df['sample_name_unique']]\n",
    "sample_df['N_CC'] = [Ncc[sn] for sn in sample_df['sample_name_unique']]\n",
    "sample_df['N_CCA'] = [Ncca[sn] for sn in sample_df['sample_name_unique']]\n",
    "#sample_df['N_CC+CCA'] = sample_df['N_CC'].values + sample_df['N_CCA'].values\n",
    "sample_df['percent_charging'] = sample_df['N_CCA'].values / (sample_df['N_CC'].values + sample_df['N_CCA'].values) *100\n",
    "\n",
    "AR_file_df.to_excel('index-pair_stats.xlsx')\n",
    "sample_df.to_excel('sample_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(sample_fastq_dir + '/index-pair_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../projects/pilot_exp/read_processing_stats/sample_UMI_stats.xlsx'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate UMI stats and write final trimmed tRNA sequences ###\n",
    "# Note, the cDNA input amount is so large that it is very unlikely to sequence\n",
    "# the same PCR amplified DNA twice. Therefore, this processing step does not\n",
    "# attempt to merge possible UMI duplicates.\n",
    "\n",
    "# From: https://stats.stackexchange.com/questions/296005/the-expected-number-of-unique-elements-drawn-with-replacement\n",
    "# I get the expected number of unique UMIs:\n",
    "# E_X = n*(1-((n-1) / n)**k)\n",
    "# Where k = is the number of sequences (draws)\n",
    "# and n = to the number of possible UMIs (bins)\n",
    "n_bins = 4**9 * 2 # number of UMI bins (n)\n",
    "\n",
    "\n",
    "# Create folder for files:\n",
    "try:\n",
    "    os.mkdir(umi_dir)\n",
    "except:\n",
    "    shutil.rmtree(umi_dir)\n",
    "    os.mkdir(umi_dir)\n",
    "os.chdir(umi_dir)\n",
    "\n",
    "# Trim UMIs off sequences:\n",
    "N_umi_obs = list()\n",
    "N_umi_exp = list()\n",
    "N_seq_list = list()\n",
    "for index, row in sample_df.iterrows(): # Process each sample individually\n",
    "    fastq_name = '../{}/{}.fastq.bz2'.format(sample_fastq_dir, row['sample_name_unique'])\n",
    "    UMIs = set()\n",
    "    Nseqs = 0\n",
    "    with bz2.open('{}_UMI-trimmed.fastq.bz2'.format(row['sample_name_unique']), \"wt\") as output_fh:\n",
    "        with bz2.open(fastq_name, \"rt\") as input_fh:\n",
    "            for title, seq, qual in FastqGeneralIterator(input_fh):\n",
    "                umi = seq[0:10]\n",
    "                if umi[-1] == 'T' or umi[-1] == 'C': # UMI sequence requirement\n",
    "                    UMIs.add(umi)\n",
    "                    Nseqs += 1\n",
    "                    # Write the trimmed sequence:\n",
    "                    output_fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[10:], qual[10:]))\n",
    "    # Calculate the observed and expected UMI count:\n",
    "    N_seq_list.append(Nseqs)\n",
    "    k_draws = Nseqs\n",
    "    N_umi_obs.append(len(UMIs))\n",
    "    E_X = n_bins*(1-((n_bins-1) / n_bins)**k_draws)\n",
    "    N_umi_exp.append(round(E_X))\n",
    "\n",
    "# Collect UMI stats:\n",
    "sample_df['N_UMI_observed'] = N_umi_obs\n",
    "sample_df['N_UMI_expected'] = N_umi_exp\n",
    "sample_df['percent_seqs_after_UMI_trim'] = np.array(N_seq_list) / sample_df['N_total'].values * 100\n",
    "sample_df['percent_UMI_obs-vs-exp'] = sample_df['N_UMI_observed'].values / sample_df['N_UMI_expected'].values * 100\n",
    "sample_df.to_excel('sample_UMI_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(umi_dir + '/sample_UMI_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
