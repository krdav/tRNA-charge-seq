{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq/1-fastq_processing\n"
     ]
    }
   ],
   "source": [
    "import os, sys, shutil, bz2\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO, bgzf\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "\n",
    "# Navigate back to workbookDir in case of re-running a code block:\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "os.chdir(workbookDir)  # If you changed the current working dir, this will take you back to the workbook dir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "gggg\n",
    "ssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "### Need a method to validate adapter barcode uniqueness\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These folder/files change depending on dataset:\n",
    "#data_folder = 'data/pilot_exp'\n",
    "#project_folder = 'projects/pilot_exp'\n",
    "data_folder = 'data/pilot_exp_v3'\n",
    "project_folder = 'projects/pilot_exp_v3'\n",
    "seq_folder = 'raw_fastq'\n",
    "index_list = 'index_list.xlsx'\n",
    "sample_list = 'sample_list.xlsx'\n",
    "\n",
    "# Define minimum read length based on minimum insert size:\n",
    "MIN_INSERT = 15\n",
    "UMI_LEN = 10\n",
    "BC_MAX_LEN = 12\n",
    "MIN_READ_LEN = MIN_INSERT + UMI_LEN + BC_MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These folder names are used in subsequent steps, so do not change:\n",
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "sample_fastq_dir = 'processed_fastq'\n",
    "umi_dir = 'UMI_trimmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample/index information:\n",
    "index_df = pd.read_excel(index_list)\n",
    "sample_df = pd.read_excel('../' + project_folder + '/' + sample_list)\n",
    "\n",
    "# Make folder structure for data and stats:\n",
    "os.chdir('../' + data_folder)\n",
    "stats_dir = '../../' + project_folder + '/read_processing_stats'\n",
    "try:\n",
    "    os.mkdir(stats_dir)\n",
    "except:\n",
    "    shutil.rmtree(stats_dir)\n",
    "    os.mkdir(stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read index sequences into dict:\n",
    "index_dict = dict()\n",
    "for t, i, s in zip(index_df['type'].values, index_df['id'].values, index_df['sequence'].values):\n",
    "    if t not in index_dict:\n",
    "        index_dict[t] = dict()\n",
    "    index_dict[t][i] = s\n",
    "\n",
    "# Add index sequences to dataframe:\n",
    "sample_df['P5_index_seq'] = [index_dict['P5_index'][i] for i in sample_df['P5_index'].values]\n",
    "sample_df['P7_index_seq'] = [index_dict['P7_index'][i] for i in sample_df['P7_index'].values]\n",
    "sample_df['barcode_seq'] = [index_dict['barcode'][i] for i in sample_df['barcode'].values]\n",
    "\n",
    "# Add seq_folder to filename:\n",
    "sample_df['fastq_mate1_filename'] = [seq_folder + '/' + fn for fn in sample_df['fastq_mate1_filename'].values]\n",
    "sample_df['fastq_mate2_filename'] = [seq_folder + '/' + fn for fn in sample_df['fastq_mate2_filename'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/Users/krdav/anaconda3/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../projects/pilot_exp_v3/read_processing_stats/merge_stats.xlsx'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### AdapterRomoval and paired end read merging ###\n",
    "\n",
    "adapter1_tmp = 'AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC<P7_index>ATCTCGTATGCCGTCTTCTGCTTG'\n",
    "adapter2_tmp = 'AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT<P5_index>GTGTAGATCTCGGTGGTCGCCGTATCATT'\n",
    "AR_cmd_tmp = [\"AdapterRemoval\", \"--bzip2\", \"--preserve5p\", \"--collapse\", \"--minalignmentlength\", \"10\", \"--threads\", \"12\"]\n",
    "\n",
    "# Create folder for files:\n",
    "try:\n",
    "    os.mkdir(AdapterRemoval_dir)\n",
    "except:\n",
    "    shutil.rmtree(AdapterRemoval_dir)\n",
    "    os.mkdir(AdapterRemoval_dir)\n",
    "os.chdir(AdapterRemoval_dir)\n",
    "\n",
    "# Generate list of files to merge:\n",
    "AR_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].drop_duplicates()\n",
    "\n",
    "# Check files exists before starting:\n",
    "for index, row in AR_file_df.iterrows():\n",
    "    basename = '{}-{}'.format(row['P5_index'], row['P7_index'])\n",
    "    assert(os.path.exists('../{}'.format(row['fastq_mate1_filename'])))\n",
    "    assert(os.path.exists('../{}'.format(row['fastq_mate2_filename'])))\n",
    "\n",
    "# Merge files:\n",
    "N_pairs = list()\n",
    "N_merged = list()\n",
    "for index, row in AR_file_df.iterrows():\n",
    "    AR_cmd = AR_cmd_tmp.copy()\n",
    "    basename = '{}-{}'.format(row['P5_index'], row['P7_index'])\n",
    "    adapter1 = adapter1_tmp.replace('<P7_index>', row['P7_index_seq'])\n",
    "    adapter2 = adapter2_tmp.replace('<P5_index>', row['P5_index_seq'])\n",
    "\n",
    "    AR_cmd.extend(['--adapter1', adapter1])\n",
    "    AR_cmd.extend(['--adapter2', adapter2])\n",
    "    AR_cmd.extend(['--basename', basename])\n",
    "    AR_cmd.extend(['--file1', '../{}'.format(row['fastq_mate1_filename'])])\n",
    "    AR_cmd.extend(['--file2', '../{}'.format(row['fastq_mate2_filename'])])\n",
    "\n",
    "    with Popen(AR_cmd, stdout=PIPE, stderr=STDOUT, bufsize=1) as p, open('logfile.txt', 'a') as file:\n",
    "        file.write('Starting subprocess with command:')\n",
    "        file.write(str(AR_cmd))\n",
    "        file.write('\\n')\n",
    "        for line in p.stdout: # b'\\n'-separated lines\n",
    "            #sys.stdout.write(line) # pass bytes as is\n",
    "            file.write(line.decode('utf-8'))\n",
    "        file.write('\\n****** DONE ******\\n\\n\\n')\n",
    "\n",
    "    with open('{}.settings'.format(basename), 'r') as fh:\n",
    "        for line in fh:\n",
    "            if 'Total number of read pairs:' in line:\n",
    "                N_pairs.append(int(line.split(':')[1][1:]))\n",
    "            if 'Number of full-length collapsed pairs:' in line:\n",
    "                N_merged.append(int(line.split(':')[1][1:]))\n",
    "\n",
    "# Write stats:\n",
    "AR_file_df['N_pairs'] = N_pairs\n",
    "AR_file_df['N_merged'] = N_merged\n",
    "AR_file_df['percent_successfully_merged'] = AR_file_df['N_merged'].values / AR_file_df['N_pairs'].values *100\n",
    "AR_file_df.to_excel('merge_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(AdapterRemoval_dir + '/merge_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../projects/pilot_exp_v3/read_processing_stats/index-pair_stats.xlsx'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Splitting into files based on barcode ###\n",
    "\n",
    "# Create folder for files:\n",
    "try:\n",
    "    os.mkdir(sample_fastq_dir)\n",
    "except:\n",
    "    shutil.rmtree(sample_fastq_dir)\n",
    "    os.mkdir(sample_fastq_dir)\n",
    "os.chdir(sample_fastq_dir)\n",
    "\n",
    "\n",
    "# Map barcode sequences to reads:\n",
    "Nmapped = list()\n",
    "Nunmapped = list()\n",
    "Ncc = {k:0 for k in sample_df['sample_name_unique'].values}\n",
    "Ncca = {k:0 for k in sample_df['sample_name_unique'].values}\n",
    "Ntot = {k:0 for k in sample_df['sample_name_unique'].values}\n",
    "# Dump all the sequences where no barcode was found:\n",
    "unmapped_fh = bz2.open('no-barcode_untrimmed.fastq.bz2', \"wt\")\n",
    "\n",
    "for index, row in AR_file_df.iterrows(): # Pull out each merged fastq file\n",
    "    basename = '{}-{}'.format(row['P5_index'], row['P7_index'])\n",
    "    merged_fastq_fn = '../{}/{}.collapsed.bz2'.format(AdapterRemoval_dir, basename)\n",
    "    \n",
    "    # List the barcodes and associated sample names:\n",
    "    mask = (sample_df['P5_index'] == row['P5_index']) & (sample_df['P7_index'] == row['P7_index'])\n",
    "    bc_fh = [(k, v, bz2.open('{}.fastq.bz2'.format(v), \"wt\")) for k, v in zip(sample_df[mask]['barcode_seq'].values, sample_df[mask]['sample_name_unique'].values)]\n",
    "    \n",
    "    # Iterate over each record in the fastq file:\n",
    "    with bz2.open(merged_fastq_fn, \"rt\") as input_fh:\n",
    "        Nmapped.append(0)\n",
    "        Nunmapped.append(0)\n",
    "        for title, seq, qual in FastqGeneralIterator(input_fh):\n",
    "            # Search for barcodes and write to barcode specific file:\n",
    "            found = False\n",
    "            for bc, sample_name, fh in bc_fh:\n",
    "                if all(l1==l2 for l1, l2 in zip(seq[-len(bc):], bc) if l2 != 'N'):\n",
    "                    found = True\n",
    "                    # Add barcode sequence to title:\n",
    "                    title = title + ':' + seq[-len(bc):]\n",
    "                    fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[:-len(bc)], qual[:-len(bc)]))\n",
    "                    Nmapped[-1] += 1\n",
    "                    Ntot[sample_name] += 1\n",
    "                    # Count if CC, CCA or not:\n",
    "                    if seq[-(len(bc)+2):-len(bc)] == 'CC':\n",
    "                        Ncc[sample_name] += 1\n",
    "                    elif seq[-(len(bc)+3):-len(bc)] == 'CCA':\n",
    "                        Ncca[sample_name] += 1\n",
    "                    break\n",
    "            if not found:\n",
    "                Nunmapped[-1] += 1\n",
    "                unmapped_fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq, qual))\n",
    "    for bc, sample_name, fh in bc_fh:\n",
    "        fh.close()\n",
    "unmapped_fh.close()\n",
    "\n",
    "# Collect stats:\n",
    "AR_file_df['N_BC-mapped'] = Nmapped\n",
    "AR_file_df['N_BC-unmapped'] = Nunmapped\n",
    "AR_file_df['N_sum-check'] = AR_file_df['N_BC-mapped'] + AR_file_df['N_BC-unmapped']\n",
    "AR_file_df['percent_BC-mapped'] = AR_file_df['N_BC-mapped'].values / AR_file_df['N_merged'].values *100\n",
    "\n",
    "sample_df['N_total'] = [Ntot[sn] for sn in sample_df['sample_name_unique']]\n",
    "sample_df['N_CC'] = [Ncc[sn] for sn in sample_df['sample_name_unique']]\n",
    "sample_df['N_CCA'] = [Ncca[sn] for sn in sample_df['sample_name_unique']]\n",
    "sample_df['N_CCA+CC'] = sample_df['N_CCA'].values + sample_df['N_CC'].values\n",
    "sample_df['CCA+CC_percent_total'] = sample_df['N_CCA+CC'].values / sample_df['N_total'].values *100\n",
    "sample_df['percent_CCA'] = sample_df['N_CCA'].values / sample_df['N_CCA+CC'].values *100\n",
    "\n",
    "AR_file_df.to_excel('index-pair_stats.xlsx')\n",
    "sample_df.to_excel('sample_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(sample_fastq_dir + '/index-pair_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find Kmers for unmapped reads ###\n",
    "\n",
    "os.chdir(sample_fastq_dir)\n",
    "\n",
    "def add_kmers(k_dict, seq, k_size, filter_dict=None):\n",
    "    '''Find Kmers in input sequence and add to dictionary if not filtered.'''\n",
    "    for i in range(len(seq) - k_size + 1):\n",
    "        kmer = seq[i:(i+k_size)]\n",
    "        if filter_dict is None or kmer not in filter_dict:\n",
    "            try:\n",
    "                k_dict[kmer] += 1\n",
    "            except KeyError:\n",
    "                k_dict[kmer] = 1\n",
    "    return(k_dict)\n",
    "\n",
    "def find_min_dist_bc(kmer_seq, index_dict):\n",
    "    '''Search for the Kmers in the adapter sequences.'''\n",
    "    import jellyfish\n",
    "    dist_min = 999\n",
    "    bc_min_dist = ''\n",
    "    for bc, bc_seq in index_dict['barcode'].items():\n",
    "        for i in range(len(bc_seq) - len(kmer_seq) + 1):\n",
    "            window = bc_seq[i:(i+len(kmer_seq))]\n",
    "            dist = jellyfish.hamming_distance(window, kmer_seq)\n",
    "            if dist < dist_min:\n",
    "                dist_min = dist\n",
    "                bc_min_dist = bc\n",
    "    return(bc_min_dist, dist_min)\n",
    "\n",
    "# Search for Kmers #\n",
    "k_size = 5 # Size of Kmer\n",
    "\n",
    "# First generate a filter composed of the Kmers contained\n",
    "# in the last 7 nt. of human tRNA seqeunces.\n",
    "filter_search_size = 7\n",
    "filter_dict = dict()\n",
    "tRNA_database = '../../../2-align_reads/tRNA_database/human/hg38-tRNAs.fa'\n",
    "with open(tRNA_database, \"r\") as tRNA_fh:\n",
    "    for tRNA in SeqIO.parse(tRNA_fh, \"fasta\"):\n",
    "        filter_dict = add_kmers(filter_dict, str(tRNA.seq)[-filter_search_size:], k_size)\n",
    "\n",
    "# Then search for Kmers in the last 13 nt. in reads\n",
    "# longer than the minimum cutoff:\n",
    "search_size = 13\n",
    "k_dict = dict()\n",
    "with bz2.open('no-barcode_untrimmed.fastq.bz2', \"rt\") as unmapped_fh:\n",
    "    for title, seq, qual in FastqGeneralIterator(unmapped_fh):\n",
    "        if len(seq) >= MIN_READ_LEN:\n",
    "            k_dict = add_kmers(k_dict, seq[-search_size:], k_size, filter_dict)\n",
    "\n",
    "# Rank Kmers by occurence and find closely related adapters: \n",
    "kmer_df_dat = list()\n",
    "for kmer_seq, count in sorted(k_dict.items(), key=lambda x:x[1], reverse=True):\n",
    "    bc_min_dist, dist_min = find_min_dist_bc(kmer_seq, index_dict)\n",
    "    if dist_min < 2:\n",
    "        kmer_df_dat.append([kmer_seq, count, dist_min, bc_min_dist])\n",
    "    else:\n",
    "        kmer_df_dat.append([kmer_seq, count, None, None])\n",
    "kmer_df = pd.DataFrame(kmer_df_dat, columns=['Kmer', 'Count', 'Barcode distance', 'Barcode'])\n",
    "kmer_df.to_excel('no-barcode_Kmer-analysis.xlsx')\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../projects/pilot_exp_v3/read_processing_stats/sample_UMI_stats.xlsx'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate UMI stats and write final trimmed tRNA sequences ###\n",
    "# Note, the cDNA input amount is so large that it is very unlikely to sequence\n",
    "# the same PCR amplified DNA twice. Therefore, this processing step does not\n",
    "# attempt to merge possible UMI duplicates.\n",
    "\n",
    "# From: https://stats.stackexchange.com/questions/296005/the-expected-number-of-unique-elements-drawn-with-replacement\n",
    "# I get the expected number of unique UMIs:\n",
    "# E_X = n*(1-((n-1) / n)**k)\n",
    "# Where k = is the number of sequences (draws)\n",
    "# and n = to the number of possible UMIs (bins)\n",
    "n_bins = 4**9 * 2 # number of UMI bins (n)\n",
    "\n",
    "\n",
    "# Create folder for files:\n",
    "try:\n",
    "    os.mkdir(umi_dir)\n",
    "except:\n",
    "    shutil.rmtree(umi_dir)\n",
    "    os.mkdir(umi_dir)\n",
    "os.chdir(umi_dir)\n",
    "\n",
    "# Dump all the sequences where no UMI was found:\n",
    "unmapped_fh = bz2.open('no-UMI_untrimmed.fastq.bz2', \"wt\")\n",
    "\n",
    "# Trim UMIs off sequences:\n",
    "N_umi_obs = list()\n",
    "N_umi_exp = list()\n",
    "N_seq_list = list()\n",
    "for index, row in sample_df.iterrows(): # Process each sample individually\n",
    "    fastq_name = '../{}/{}.fastq.bz2'.format(sample_fastq_dir, row['sample_name_unique'])\n",
    "    UMIs = set()\n",
    "    Nseqs = 0\n",
    "    with bz2.open('{}_UMI-trimmed.fastq.bz2'.format(row['sample_name_unique']), \"wt\") as output_fh:\n",
    "        with bz2.open(fastq_name, \"rt\") as input_fh:\n",
    "            for title, seq, qual in FastqGeneralIterator(input_fh):\n",
    "                umi = seq[0:UMI_LEN]\n",
    "                if umi[-1] == 'T' or umi[-1] == 'C': # UMI sequence requirement\n",
    "                    UMIs.add(umi)\n",
    "                    Nseqs += 1\n",
    "                    # Add UMI sequence to title:\n",
    "                    title = title + ':' + umi\n",
    "                    # Write the trimmed sequence:\n",
    "                    output_fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[10:], qual[10:]))\n",
    "                else:\n",
    "                    # Write the untrimmed sequence if UMI was not found:\n",
    "                    unmapped_fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq, qual))\n",
    "    # Calculate the observed and expected UMI count:\n",
    "    N_seq_list.append(Nseqs)\n",
    "    k_draws = Nseqs\n",
    "    N_umi_obs.append(len(UMIs))\n",
    "    E_X = n_bins*(1-((n_bins-1) / n_bins)**k_draws)\n",
    "    N_umi_exp.append(round(E_X))\n",
    "\n",
    "# Collect UMI stats:\n",
    "sample_df['N_UMI_observed'] = N_umi_obs\n",
    "sample_df['N_UMI_expected'] = N_umi_exp\n",
    "sample_df['percent_seqs_after_UMI_trim'] = np.array(N_seq_list) / sample_df['N_total'].values * 100\n",
    "sample_df['percent_UMI_obs-vs-exp'] = sample_df['N_UMI_observed'].values / sample_df['N_UMI_expected'].values * 100\n",
    "sample_df.to_excel('sample_UMI_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(umi_dir + '/sample_UMI_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
