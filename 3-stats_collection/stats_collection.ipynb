{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq/3-stats_collection\n"
     ]
    }
   ],
   "source": [
    "import sys, os, subprocess, shutil, glob, bz2, json\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "palette = list(mcolors.TABLEAU_COLORS.keys())\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Navigate back to workbookDir in case of re-running a code block:\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "os.chdir(workbookDir)  # If you changed the current working dir, this will take you back to the workbook dir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "1. gfsa\n",
    "2. hgrewsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNAcompRNA = {a: b for a, b in zip('ATGC', 'UACG')}\n",
    "def anticodon2codon(anticodon):\n",
    "    codon = ''.join([DNAcompRNA[b] for b in anticodon[::-1]])\n",
    "    return(codon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that changes from batch to batch:\n",
    "CLEAN_DIR = False  # Delete old stats_collection dir\n",
    "OVERWRITE = True   # Overwrite old stat file\n",
    "# Following, not implemented\n",
    "DRY_RUN = False     # Do dry-run, print files to run, nothing else\n",
    "SP_SET = {'mouse', 'human'} # Only run if species is in set\n",
    "\n",
    "\n",
    "#data_folder = 'data/pilot_exp'\n",
    "data_folder = 'data/pilot_exp_v2'\n",
    "# project_folder = 'projects/pilot_exp'\n",
    "project_folder = 'projects/pilot_exp_v2'\n",
    "tRNA_database = dict()\n",
    "tRNA_database['human'] = '../../../2-align_reads/tRNA_database/human/hg38-tRNAs.fa'\n",
    "tRNA_database['mouse'] = '../../../2-align_reads/tRNA_database/mouse/mm10-tRNAs.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that should not change:\n",
    "data_dir = 'stats_collection'\n",
    "align_dir = 'SWalign'\n",
    "umi_dir = 'UMI_trimmed'\n",
    "sample_list = 'sample_list.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read sample information ###\n",
    "sample_df = pd.read_excel('../' + project_folder + '/' + sample_list)\n",
    "sample_dict = {un: {'sample_name': n, 'replicate': r, 'barcode': b, 'species': sp} for un, n, r, b, sp in zip(sample_df['sample_name_unique'].values, sample_df['sample_name'].values, sample_df['replicate'].values, sample_df['barcode'].values, sample_df['species'].values)}\n",
    "\n",
    "# Create folder for data and stats:\n",
    "os.chdir('../' + data_folder)\n",
    "stats_dir = '../../' + project_folder + '/stats_collection'\n",
    "try:\n",
    "    os.mkdir(stats_dir) # For stats\n",
    "except:\n",
    "    if CLEAN_DIR:\n",
    "        shutil.rmtree(stats_dir)\n",
    "        os.mkdir(stats_dir)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# For manipulations and final data:\n",
    "try:\n",
    "    os.mkdir(data_dir) # For data\n",
    "except:\n",
    "    if CLEAN_DIR:\n",
    "        shutil.rmtree(data_dir)\n",
    "        os.mkdir(data_dir)\n",
    "    else:\n",
    "        pass\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the tRNA database to find the length of each sequence:\n",
    "tRNA_data = dict()\n",
    "for species in tRNA_database:\n",
    "    for record in SeqIO.parse(tRNA_database[species], \"fasta\"):\n",
    "        tRNA_data[record.id] = dict()\n",
    "        tRNA_data[record.id]['len'] = len(record.seq)\n",
    "        tRNA_data[record.id]['codon'] = anticodon2codon(record.id.split('-')[2])\n",
    "        tRNA_data[record.id]['anticodon'] = record.id.split('-')[2]\n",
    "        tRNA_data[record.id]['amino_acid'] = record.id.split('-')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 100p1\n",
      "Processing: 75p2\n",
      "Processing: 25p4\n",
      "Processing: 25p2\n",
      "Processing: 0p3\n",
      "Processing: 100p4N\n",
      "Processing: 25p2N\n",
      "Processing: 0p3N\n",
      "Processing: 50p3N\n",
      "Processing: 100p4\n",
      "Processing: 100p1N\n",
      "Processing: 50p1\n",
      "Processing: 50p1N\n",
      "Processing: 25p4N\n",
      "Processing: 50p3\n",
      "Processing: 75p2N\n"
     ]
    }
   ],
   "source": [
    "stat_csv_fnam = 'stats_collection.csv.bz2'\n",
    "agg_csv_fnam = 'stats_filtered_CC-CCA-aggregate.csv'\n",
    "agg_strict_csv_fnam = 'stats_strict_filtered_CC-CCA-aggregate.csv'\n",
    "if OVERWRITE:\n",
    "    try:\n",
    "        os.remove(stat_csv_fnam)\n",
    "        os.remove(agg_csv_fnam)\n",
    "        os.remove(agg_strict_csv_fnam)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Open filehandles and printer headers:\n",
    "fh_stats_out = bz2.open(stat_csv_fnam, 'ab')\n",
    "header = ['readID', 'sample_name', 'replicate', 'barcode', 'tRNA_annotation', 'align_score', 'unique_annotation', 'tRNA_annotation_len', 'align_5p_idx', 'align_3p_idx', 'align_5p_nt', 'align_3p_nt', 'codon', 'anticodon', 'amino_acid', '5p_cover', '3p_cover', '5p_non-temp', '3p_non-temp', '5p_UMI', '3p_BC']\n",
    "header_bin = str.encode(','.join(header) + '\\n')\n",
    "fh_stats_out.write(header_bin)\n",
    "\n",
    "fh_agg_out = open(agg_csv_fnam, 'a')\n",
    "fh_agg_strict_out = open(agg_strict_csv_fnam, 'a')\n",
    "agg_cols = ['sample_name', 'replicate', 'barcode', 'tRNA_annotation', 'tRNA_annotation_len', 'unique_annotation', 'align_3p_nt', 'codon', 'anticodon', 'amino_acid', 'count']\n",
    "print(','.join(agg_cols), file=fh_agg_out)\n",
    "print(','.join(agg_cols), file=fh_agg_strict_out)\n",
    "\n",
    "# Files to collect stats from:\n",
    "json_files = glob.glob('../' + align_dir + '/*.json.bz2')\n",
    "json_files.sort(key=os.path.getmtime)\n",
    "# print(json_files)\n",
    "for json_file in json_files:\n",
    "    fnam_base = '_'.join(json_file.split('/')[-1].split('_')[0:-1])\n",
    "    uidx = fnam_base.index('UMI')\n",
    "    unique_sample_name = fnam_base[0:uidx-1]\n",
    "    print('Processing: {}'.format(unique_sample_name))\n",
    "    with bz2.open(json_file, 'rt', encoding=\"utf-8\") as fh_gz:\n",
    "        SWres = json.load(fh_gz)\n",
    "    \n",
    "    # Extract non-template bases from UMI processed reads:\n",
    "    fastq_fnam = '../' + umi_dir + '/' + fnam_base + '.fastq.bz2'\n",
    "    with bz2.open(fastq_fnam, 'rt') as fh_gz:\n",
    "        for UMIread in SeqIO.parse(fh_gz, \"fastq\"):\n",
    "            if UMIread.id in SWres:\n",
    "                qpos = SWres[UMIread.id]['qpos'][0]\n",
    "                SWres[UMIread.id]['5p_non-temp'] = str(UMIread.seq)[0:(qpos[0]-1)]\n",
    "                SWres[UMIread.id]['3p_non-temp'] = str(UMIread.seq)[qpos[1]:]\n",
    "                _3p_bc, _5p_umi = UMIread.description.split()[-1].split(':')[-2:]\n",
    "                SWres[UMIread.id]['5p_UMI'] = _5p_umi\n",
    "                SWres[UMIread.id]['3p_BC'] = _3p_bc\n",
    "\n",
    "    ### Collect stats on a per sample basis and store in tmp file ###\n",
    "    with open('tmp_stat.csv', 'w') as tmp_csv:\n",
    "        print(','.join(header), file=tmp_csv)\n",
    "        for i, readID in enumerate(SWres):\n",
    "            try:\n",
    "                SWres[readID]['5p_non-temp']\n",
    "            except KeyError:\n",
    "                raise Exception('5p_non-temp was not defined in SWres[readID]. Should have been added from the UMI trimmed reads. Did any of the fastq headers change such that there is a mismatch between headers in the alignment json and those in the trimmed UMIs?')\n",
    "            sample_name = sample_dict[unique_sample_name]['sample_name']\n",
    "            replicate = sample_dict[unique_sample_name]['replicate']\n",
    "            barcode = sample_dict[unique_sample_name]['barcode']\n",
    "            tRNA_annotation = SWres[readID]['name']\n",
    "            tRNA_annotation_first = tRNA_annotation.split('@')[0]\n",
    "            align_score = SWres[readID]['score']\n",
    "            unique_annotation = '@' not in tRNA_annotation\n",
    "            tRNA_annotation_len = tRNA_data[tRNA_annotation_first]['len']\n",
    "            align_5p_idx, align_3p_idx = SWres[readID]['dpos'][0]\n",
    "            align_5p_nt = SWres[readID]['qseq'][0]\n",
    "            align_3p_nt = SWres[readID]['qseq'][-1]\n",
    "            # Move index for reads with beta-eliminated A:\n",
    "            if align_3p_idx == (tRNA_annotation_len - 1) and align_3p_nt == 'C':\n",
    "                align_3p_idx += 1\n",
    "            codon = tRNA_data[tRNA_annotation_first]['codon']\n",
    "            anticodon = tRNA_data[tRNA_annotation_first]['anticodon']\n",
    "            amino_acid = tRNA_data[tRNA_annotation_first]['amino_acid']\n",
    "            _5p_cover = align_5p_idx == 1\n",
    "            _3p_cover = align_3p_idx == tRNA_annotation_len\n",
    "            _5p_non_temp = SWres[readID]['5p_non-temp']\n",
    "            _3p_non_temp = SWres[readID]['3p_non-temp']\n",
    "            _5p_umi = SWres[readID]['5p_UMI']\n",
    "            _3p_bc = SWres[readID]['3p_BC']\n",
    "            # Print line to tmp file:\n",
    "            csv_line = ','.join(map(str, [readID, sample_name, replicate, barcode, tRNA_annotation, align_score, unique_annotation, tRNA_annotation_len, align_5p_idx, align_3p_idx, align_5p_nt, align_3p_nt, codon, anticodon, amino_acid, _5p_cover, _3p_cover, _5p_non_temp, _3p_non_temp, _5p_umi, _3p_bc]))\n",
    "            print(csv_line, file=tmp_csv)\n",
    "    # Append the sample statistics to the final csv file using bz2 compression:\n",
    "    with open('tmp_stat.csv', 'rb') as tmp_csv:\n",
    "        next(tmp_csv) # skip header\n",
    "        fh_stats_out.write(tmp_csv.read())\n",
    "\n",
    "    ### Aggregate filtered data to count charged/uncharged tRNAs ###\n",
    "    # Read stats from tmp csv file (this is faster than building row by row):\n",
    "    stat_df = pd.read_csv('tmp_stat.csv', keep_default_na=False)\n",
    "    os.remove('tmp_stat.csv')\n",
    "    # 3' must be covered and no 3' non-template bases:\n",
    "    row_mask = (stat_df['3p_cover']) & (stat_df['3p_non-temp'] == '')\n",
    "    agg_df = stat_df.loc[row_mask, agg_cols[0:-1]]\n",
    "    agg_df['count'] = stat_df.loc[row_mask, ['align_3p_nt']]  # dummy for groupby count\n",
    "    agg_df = agg_df.groupby(agg_cols, as_index=False).agg({\"count\": \"count\"})\n",
    "    agg_df.to_csv(fh_agg_out, header=False, index=False, mode='a')\n",
    "    \n",
    "    row_mask = (stat_df['unique_annotation']) & (stat_df['5p_cover']) & (stat_df['3p_cover']) & (stat_df['3p_non-temp'] == '')\n",
    "    agg_strict_df = stat_df.loc[row_mask, agg_cols[0:-1]]\n",
    "    agg_strict_df['count'] = stat_df.loc[row_mask, ['align_3p_nt']]  # dummy for groupby count\n",
    "    agg_strict_df = agg_strict_df.groupby(agg_cols, as_index=False).agg({\"count\": \"count\"})\n",
    "    agg_strict_df.to_csv(fh_agg_strict_out, header=False, index=False, mode='a')\n",
    "\n",
    "fh_stats_out.close()\n",
    "fh_agg_out.close()\n",
    "fh_agg_strict_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files to collect stats from:\n",
    "json_files = glob.glob('../' + align_dir + '/*.json.bz2')\n",
    "# print(json_files)\n",
    "file_done = True\n",
    "for json_file in json_files[::-1]:\n",
    "    fnam_base = '_'.join(json_file.split('/')[-1].split('_')[0:-1])\n",
    "    uidx = fnam_base.index('UMI')\n",
    "    unique_sample_name = fnam_base[0:uidx-1]\n",
    "    if unique_sample_name != '8U2' and file_done:\n",
    "        continue\n",
    "    else:\n",
    "        file_done = False\n",
    "    \n",
    "    print('Processing: {}'.format(unique_sample_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
