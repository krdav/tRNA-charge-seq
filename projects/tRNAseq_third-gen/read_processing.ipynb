{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, bz2, random, resource, warnings\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import numpy as np\n",
    "from Bio import SeqIO, bgzf\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from mpire import WorkerPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is in: /Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq/projects/tRNAseq_third-gen\n",
      "Repo is in: /Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq\n",
      "Using minimum read length: 44 (after merge)\n"
     ]
    }
   ],
   "source": [
    "# Navigate back to NBdir in case of re-running a code block:\n",
    "if not 'NBdir' in globals():\n",
    "    NBdir = os.getcwd()\n",
    "print('Notebook is in: {}'.format(NBdir))\n",
    "os.chdir(NBdir)  # If you changed the current working dir, this will take you back to the notebook dir.\n",
    "\n",
    "# Define the path to the repo folder.\n",
    "# Change if necessary.\n",
    "homedir = '/'.join(NBdir.split('/')[0:-2])\n",
    "print('Repo is in: {}'.format(homedir))\n",
    "sys.path.insert(1, homedir)\n",
    "from utils.functions import index_to_sample_df, downsample_raw_input, AR_merge, BC_split\n",
    "\n",
    "# These are default folder names for data and raw fastq files\n",
    "# relative to the folder in which this notebook is in:\n",
    "data_folder = 'data'\n",
    "seq_folder_noDS = 'raw_fastq' # Not downsampled\n",
    "seq_folder = 'raw_fastq'\n",
    "\n",
    "# These folder names are used in subsequent processing steps\n",
    "# to dump data. Best to not change:\n",
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "BC_dir = 'BC_split'\n",
    "UMI_dir = 'UMI_trimmed'\n",
    "\n",
    "# Define minimum read length based on minimum insert size:\n",
    "MIN_INSERT = 15\n",
    "UMI_LEN = 10\n",
    "BC_MAX_LEN = 19\n",
    "MIN_READ_LEN = MIN_INSERT + UMI_LEN + BC_MAX_LEN\n",
    "print('Using minimum read length: {} (after merge)'.format(MIN_READ_LEN))\n",
    "\n",
    "\n",
    "# Read index information:\n",
    "index_list_fnam = 'index_list.xlsx'\n",
    "index_df = pd.read_excel('{}/utils/{}'.format(homedir, index_list_fnam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list_fnam = 'sample_list_P1-2-3.xlsx'\n",
    "sample_df = pd.read_excel('{}/{}'.format(NBdir, sample_list_fnam))\n",
    "# Add barcode sequences:\n",
    "sample_df = index_to_sample_df(sample_df, index_df)\n",
    "# Get filenames from the sample information:\n",
    "inp_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].copy().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Downsample:\n",
    "if True:\n",
    "    downsample_fold = 100\n",
    "    downsample_absolute = 1e5 # Takes precedence, False if fold should be used\n",
    "    sample_df, inp_file_df, seq_folder = downsample_raw_input(sample_df, inp_file_df, NBdir, data_folder, seq_folder_noDS, downsample_absolute=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AdapterRemoval:\n",
    "AR_obj = AR_merge(sample_df, inp_file_df, NBdir, data_folder, seq_folder, AdapterRemoval_dir, MIN_READ_LEN)\n",
    "AdapterRemoval_dir_abs = AR_obj.make_dir(overwrite=True)\n",
    "# inp_file_df = AR_obj.run_serial()\n",
    "inp_file_df = AR_obj.run_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split files based on barcodes:\n",
    "BCsplit_obj = BC_split(sample_df, inp_file_df, NBdir, data_folder, AdapterRemoval_dir_abs, BC_dir)\n",
    "BC_dir_abs = BCsplit_obj.make_dir(overwrite=True)\n",
    "# sample_df, inp_file_df = BCsplit_obj.run_serial()\n",
    "sample_df, inp_file_df = BCsplit_obj.run_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find Kmers for unmapped reads ###\n",
    "\n",
    "os.chdir(sample_fastq_dir)\n",
    "\n",
    "def add_kmers(k_dict, seq, k_size, filter_dict=None):\n",
    "    '''Find Kmers in input sequence and add to dictionary if not filtered.'''\n",
    "    for i in range(len(seq) - k_size + 1):\n",
    "        kmer = seq[i:(i+k_size)]\n",
    "        if filter_dict is None or kmer not in filter_dict:\n",
    "            try:\n",
    "                k_dict[kmer] += 1\n",
    "            except KeyError:\n",
    "                k_dict[kmer] = 1\n",
    "    return(k_dict)\n",
    "\n",
    "def find_min_dist_bc(kmer_seq, index_dict):\n",
    "    '''Search for the Kmers in the adapter sequences.'''\n",
    "    import jellyfish\n",
    "    dist_min = 999\n",
    "    bc_min_dist = ''\n",
    "    for bc, bc_seq in index_dict['barcode'].items():\n",
    "        for i in range(len(bc_seq) - len(kmer_seq) + 1):\n",
    "            window = bc_seq[i:(i+len(kmer_seq))]\n",
    "            dist = jellyfish.hamming_distance(window, kmer_seq)\n",
    "            if dist < dist_min:\n",
    "                dist_min = dist\n",
    "                bc_min_dist = bc\n",
    "    return(bc_min_dist, dist_min)\n",
    "\n",
    "# Search for Kmers #\n",
    "k_size = 5 # Size of Kmer\n",
    "\n",
    "# First generate a filter composed of the Kmers contained\n",
    "# in the last 7 nt. of human tRNA seqeunces.\n",
    "filter_search_size = 7\n",
    "filter_dict = dict()\n",
    "tRNA_database = '../../../2-align_reads/tRNA_database/human/hg38-tRNAs.fa'\n",
    "with open(tRNA_database, \"r\") as tRNA_fh:\n",
    "    for tRNA in SeqIO.parse(tRNA_fh, \"fasta\"):\n",
    "        filter_dict = add_kmers(filter_dict, str(tRNA.seq)[-filter_search_size:], k_size)\n",
    "\n",
    "# Then search for Kmers in the last 13 nt. in reads\n",
    "# longer than the minimum cutoff:\n",
    "search_size = 13\n",
    "k_dict = dict()\n",
    "with bz2.open('no-barcode_untrimmed.fastq.bz2', \"rt\") as unmapped_fh:\n",
    "    for title, seq, qual in FastqGeneralIterator(unmapped_fh):\n",
    "        if len(seq) >= MIN_READ_LEN:\n",
    "            k_dict = add_kmers(k_dict, seq[-search_size:], k_size, filter_dict)\n",
    "\n",
    "# Rank Kmers by occurence and find closely related adapters: \n",
    "kmer_df_dat = list()\n",
    "for kmer_seq, count in sorted(k_dict.items(), key=lambda x:x[1], reverse=True):\n",
    "    bc_min_dist, dist_min = find_min_dist_bc(kmer_seq, index_dict)\n",
    "    if dist_min < 2:\n",
    "        kmer_df_dat.append([kmer_seq, count, dist_min, bc_min_dist])\n",
    "    else:\n",
    "        kmer_df_dat.append([kmer_seq, count, None, None])\n",
    "kmer_df = pd.DataFrame(kmer_df_dat, columns=['Kmer', 'Count', 'Barcode distance', 'Barcode'])\n",
    "kmer_df.to_excel('no-barcode_Kmer-analysis.xlsx')\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../projects/tRNAseq_third-gen/read_processing_stats/sample_UMI_stats.xlsx'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate UMI stats and write final trimmed tRNA sequences ###\n",
    "# Note, the cDNA input amount is so large that it is very unlikely to sequence\n",
    "# the same PCR amplified DNA twice. Therefore, this processing step does not\n",
    "# attempt to merge possible UMI duplicates.\n",
    "\n",
    "# From: https://stats.stackexchange.com/questions/296005/the-expected-number-of-unique-elements-drawn-with-replacement\n",
    "# I get the expected number of unique UMIs:\n",
    "# E_X = n*(1-((n-1) / n)**k)\n",
    "# Where k = is the number of sequences (draws)\n",
    "# and n = to the number of possible UMIs (bins)\n",
    "n_bins = 4**9 * 2 # number of UMI bins (n)\n",
    "\n",
    "\n",
    "# Create folder for files:\n",
    "try:\n",
    "    os.mkdir(umi_dir)\n",
    "except:\n",
    "    shutil.rmtree(umi_dir)\n",
    "    os.mkdir(umi_dir)\n",
    "os.chdir(umi_dir)\n",
    "\n",
    "# Dump all the sequences where no UMI was found:\n",
    "unmapped_fh = bz2.open('no-UMI_untrimmed.fastq.bz2', \"wt\")\n",
    "\n",
    "# Trim UMIs off sequences:\n",
    "N_umi_obs = list()\n",
    "N_umi_exp = list()\n",
    "N_seq_list = list()\n",
    "for index, row in sample_df.iterrows(): # Process each sample individually\n",
    "    fastq_name = '../{}/{}.fastq.bz2'.format(sample_fastq_dir, row['sample_name_unique'])\n",
    "    UMIs = set()\n",
    "    Nseqs = 0\n",
    "    with bz2.open('{}_UMI-trimmed.fastq.bz2'.format(row['sample_name_unique']), \"wt\") as output_fh:\n",
    "        with bz2.open(fastq_name, \"rt\") as input_fh:\n",
    "            for title, seq, qual in FastqGeneralIterator(input_fh):\n",
    "                umi = seq[0:UMI_LEN]\n",
    "                if umi[-1] == 'T' or umi[-1] == 'C': # UMI sequence requirement\n",
    "                    UMIs.add(umi)\n",
    "                    Nseqs += 1\n",
    "                    # Add UMI sequence to title:\n",
    "                    title = title + ':' + umi\n",
    "                    # Write the trimmed sequence:\n",
    "                    output_fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq[10:], qual[10:]))\n",
    "                else:\n",
    "                    # Write the untrimmed sequence if UMI was not found:\n",
    "                    unmapped_fh.write(\"@{}\\n{}\\n+\\n{}\\n\".format(title, seq, qual))\n",
    "    # Calculate the observed and expected UMI count:\n",
    "    N_seq_list.append(Nseqs)\n",
    "    k_draws = Nseqs\n",
    "    N_umi_obs.append(len(UMIs))\n",
    "    E_X = n_bins*(1-((n_bins-1) / n_bins)**k_draws)\n",
    "    N_umi_exp.append(round(E_X))\n",
    "\n",
    "# Collect UMI stats:\n",
    "sample_df['N_UMI_observed'] = N_umi_obs\n",
    "sample_df['N_UMI_expected'] = N_umi_exp\n",
    "sample_df['percent_seqs_after_UMI_trim'] = np.array(N_seq_list) / sample_df['N_total'].values * 100\n",
    "sample_df['percent_UMI_obs-vs-exp'] = sample_df['N_UMI_observed'].values / sample_df['N_UMI_expected'].values * 100\n",
    "sample_df.to_excel('sample_UMI_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(umi_dir + '/sample_UMI_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
