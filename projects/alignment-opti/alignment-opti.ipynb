{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, shutil, bz2, copy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import numpy as np\n",
    "\n",
    "### Plotting imports ###\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import StepPatch\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "import logomaker as lm\n",
    "palette = list(mcolors.TABLEAU_COLORS.keys())\n",
    "sns.set_theme(style=\"ticks\", palette=\"muted\")\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is in: /home/sulab/tRNA-charge-seq/projects/alignment-opti\n",
      "Repo is in: /home/sulab/tRNA-charge-seq\n",
      "Using minimum read length: 39 (after merge)\n",
      "Using minimum alignemnt score: 15\n"
     ]
    }
   ],
   "source": [
    "# Navigate back to NBdir in case of re-running a code block:\n",
    "if not 'NBdir' in globals():\n",
    "    NBdir = os.getcwd()\n",
    "print('Notebook is in: {}'.format(NBdir))\n",
    "os.chdir(NBdir)  # If you changed the current working dir, this will take you back to the notebook dir.\n",
    "\n",
    "# Define the path to the repo folder.\n",
    "# Change if necessary.\n",
    "homedir = '/'.join(NBdir.split('/')[0:-2])\n",
    "print('Repo is in: {}'.format(homedir))\n",
    "sys.path.insert(1, homedir)\n",
    "from src.misc import index_to_sample_df, downsample_raw_input, read_tRNAdb_info, sample_df_to_dict\n",
    "from src.read_processing import AR_merge, BC_split, Kmer_analysis, BC_analysis, UMI_trim\n",
    "from src.alignment import SWIPE_align\n",
    "from src.stats_collection import STATS_collection\n",
    "from src.plotting import TRNA_plot\n",
    "from src.transcript_mutations import TM_analysis\n",
    "\n",
    "# These are default folder names for data and raw fastq files\n",
    "# relative to the folder in which this notebook is in:\n",
    "data_dir = 'data'\n",
    "seq_dir = 'raw_fastq'\n",
    "seq_dir_noDS = seq_dir # Not downsampled\n",
    "\n",
    "# These folder names are used in subsequent processing steps\n",
    "# to dump data. Best to not change:\n",
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "BC_dir = 'BC_split'\n",
    "UMI_dir = 'UMI_trimmed'\n",
    "align_dir = 'SWalign'\n",
    "stats_dir = 'stats_collection'\n",
    "TM_dir = 'transcript_mutations'\n",
    "plotting_dir = 'plotting'\n",
    "tRNA_database = dict()\n",
    "tRNA_database['human'] = '{}/tRNA_database/human/hg38-tRNAs.fa'.format(homedir)\n",
    "tRNA_database['mouse'] = '{}/tRNA_database/mouse/mm10-tRNAs.fa'.format(homedir)\n",
    "# Read information (length, codon etc) of tRNAs into dictionary:\n",
    "tRNA_data = read_tRNAdb_info(tRNA_database)\n",
    "SWIPE_score_mat = '{}/utils/nuc_score-matrix.txt'.format(homedir)\n",
    "SWIPE_score_mat2 = '{}/utils/nuc_score-matrix_2.txt'.format(homedir) # For masked reference sequences\n",
    "# tRNA sequencing yields many duplicated reads.\n",
    "# Adding these commonly seen sequences to a list prevents duplicated alignment:\n",
    "common_seqs = '{}/utils/common-seqs.fasta.bz2'.format(homedir)\n",
    "\n",
    "# Define minimum read length based on minimum insert size:\n",
    "MIN_INSERT_LEN = 10\n",
    "UMI_LEN = 10\n",
    "BC_MAX_LEN = 19\n",
    "MIN_READ_LEN = MIN_INSERT_LEN + UMI_LEN + BC_MAX_LEN\n",
    "print('Using minimum read length: {} (after merge)'.format(MIN_READ_LEN))\n",
    "\n",
    "# The minimum alignment score.\n",
    "# Better to set relatively low, since additional filtering can\n",
    "# be applied later.\n",
    "MIN_SCORE_ALIGN = 15\n",
    "print('Using minimum alignemnt score: {}'.format(MIN_SCORE_ALIGN))\n",
    "\n",
    "# Read index information:\n",
    "index_list_fnam = 'index_list.xlsx'\n",
    "index_df = pd.read_excel('{}/utils/{}'.format(homedir, index_list_fnam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list_fnam = 'sample_list_alignment-opti.xlsx'\n",
    "sample_df = pd.read_excel('{}/{}'.format(NBdir, sample_list_fnam))\n",
    "# Add barcode sequences:\n",
    "sample_df = index_to_sample_df(sample_df, index_df)\n",
    "# Read elementary info (replicate, barcode, species)\n",
    "# for each unique sample name into a dictionary:\n",
    "sample_dict = sample_df_to_dict(sample_df)\n",
    "# Get filenames from the sample information:\n",
    "inp_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].copy().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Make a dictionary with paths used for data processing:\n",
    "dir_dict = dict(NBdir = NBdir,\n",
    "                data_dir = data_dir,\n",
    "                seq_dir = seq_dir,\n",
    "                AdapterRemoval_dir = AdapterRemoval_dir,\n",
    "                BC_dir = BC_dir,\n",
    "                UMI_dir = UMI_dir,\n",
    "                align_dir = align_dir,\n",
    "                stats_dir = stats_dir,\n",
    "                TM_dir = TM_dir,\n",
    "                plotting_dir = plotting_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If restarting after shutdown,\n",
    "# no need to re-run first pass alignment:\n",
    "if True:\n",
    "    # Run AdapterRemoval:\n",
    "    AR_obj = AR_merge(dir_dict, inp_file_df, MIN_READ_LEN, overwrite_dir=True)\n",
    "    inp_file_df = AR_obj.run_parallel(n_jobs=4)\n",
    "\n",
    "    # Split files based on barcodes:\n",
    "    BCsplit_obj = BC_split(dir_dict, sample_df, inp_file_df, overwrite_dir=True)\n",
    "    sample_df, inp_file_df = BCsplit_obj.run_parallel(n_jobs=12)\n",
    "\n",
    "    # Trim UMI:\n",
    "    UMItrim_obj = UMI_trim(dir_dict, sample_df, overwrite_dir=True)\n",
    "    sample_df = UMItrim_obj.run_parallel(n_jobs=12)\n",
    "\n",
    "    # First pass alignment:\n",
    "    align_obj = SWIPE_align(dir_dict, tRNA_database, sample_df, SWIPE_score_mat, \\\n",
    "                            gap_penalty=6, extension_penalty=1, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                            common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "    sample_df = align_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "    # Collect alignment statistics:\n",
    "    stats_obj = STATS_collection(dir_dict, tRNA_data, sample_df, common_seqs=common_seqs, \\\n",
    "                                 overwrite_dir=True)\n",
    "    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "    # Write first alignment stats:\n",
    "    align_res = sample_df.loc[:, ['sample_name_unique', 'Mapping_percent', \\\n",
    "                                  'percent_single_annotation', 'percent_multiple_codons']]\n",
    "    align_res['unique_anno'] = None\n",
    "    align_res['frac_max_score'] = None\n",
    "    align_res['min_mut_freq'] = None\n",
    "    align_res['iteration'] = None\n",
    "    with open('align-opti_res.csv', 'w') as fh_res:\n",
    "        align_res.to_csv(fh_res, index=False)\n",
    "\n",
    "else:\n",
    "    # Run AdapterRemoval:\n",
    "    AR_obj = AR_merge(dir_dict, inp_file_df, MIN_READ_LEN, overwrite_dir=False)\n",
    "    inp_file_df = AR_obj.run_parallel(n_jobs=4, overwrite=False)\n",
    "\n",
    "    # Split files based on barcodes:\n",
    "    BCsplit_obj = BC_split(dir_dict, sample_df, inp_file_df, overwrite_dir=False)\n",
    "    sample_df, inp_file_df = BCsplit_obj.run_parallel(n_jobs=12, load_previous=True)\n",
    "\n",
    "    # Trim UMI:\n",
    "    UMItrim_obj = UMI_trim(dir_dict, sample_df, overwrite_dir=False)\n",
    "    sample_df = UMItrim_obj.run_parallel(n_jobs=12, load_previous=True)\n",
    "\n",
    "    # First pass alignment:\n",
    "    align_obj = SWIPE_align(dir_dict, tRNA_database, sample_df, SWIPE_score_mat, \\\n",
    "                            gap_penalty=6, extension_penalty=1, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                            common_seqs=common_seqs, overwrite_dir=False, verbose=False)\n",
    "    sample_df = align_obj.run_parallel(n_jobs=12, verbose=False, load_previous=True)\n",
    "\n",
    "    # Collect alignment statistics:\n",
    "    stats_obj = STATS_collection(dir_dict, tRNA_data, sample_df, common_seqs=common_seqs, \\\n",
    "                                 overwrite_dir=False)\n",
    "    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False, load_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a grid of alignment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_dict_masked = copy.deepcopy(dir_dict)\n",
    "dir_dict_masked['align_dir'] = 'SWalign_masked'\n",
    "dir_dict_masked['stats_dir'] = 'stats_collection_masked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of variables to test:\n",
    "min_mut_freq_grid = [0.5, 0.7, 0.75, 0.78, 0.8, \\\n",
    "                     0.82, 0.84, 0.86, 0.9]\n",
    "frac_max_score_grid = [0.90, 0.95, 1]\n",
    "unique_anno_grid = [True, False]\n",
    "N_iterations = 3\n",
    "total_combi = len(min_mut_freq_grid) * len(frac_max_score_grid) * len(unique_anno_grid) * N_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combi 1 of 162\n",
      "Running combi 2 of 162\n",
      "Running combi 3 of 162\n",
      "Running combi 4 of 162\n",
      "Running combi 5 of 162\n",
      "Running combi 6 of 162\n",
      "Running combi 7 of 162\n",
      "Running combi 8 of 162\n",
      "Running combi 9 of 162\n",
      "Running combi 10 of 162\n",
      "Running combi 11 of 162\n",
      "Running combi 12 of 162\n",
      "Running combi 13 of 162\n",
      "Running combi 14 of 162\n",
      "Running combi 15 of 162\n",
      "Running combi 16 of 162\n",
      "Running combi 17 of 162\n",
      "Running combi 18 of 162\n",
      "Running combi 19 of 162\n",
      "Running combi 20 of 162\n",
      "Running combi 21 of 162\n",
      "Running combi 22 of 162\n",
      "Running combi 23 of 162\n",
      "Running combi 24 of 162\n",
      "Running combi 25 of 162\n",
      "Running combi 26 of 162\n",
      "Running combi 27 of 162\n",
      "Running combi 28 of 162\n",
      "Running combi 29 of 162\n",
      "Running combi 30 of 162\n",
      "Running combi 31 of 162\n",
      "Running combi 32 of 162\n",
      "Running combi 33 of 162\n",
      "Running combi 34 of 162\n",
      "Running combi 35 of 162\n",
      "Running combi 36 of 162\n",
      "Running combi 37 of 162\n",
      "Running combi 38 of 162\n",
      "Running combi 39 of 162\n",
      "Running combi 40 of 162\n",
      "Running combi 41 of 162\n",
      "Running combi 42 of 162\n",
      "Running combi 43 of 162\n",
      "Running combi 44 of 162\n",
      "Running combi 45 of 162\n",
      "Running combi 46 of 162\n",
      "Running combi 47 of 162\n",
      "Running combi 48 of 162\n",
      "Running combi 49 of 162\n",
      "Running combi 50 of 162\n",
      "Running combi 51 of 162\n",
      "Running combi 52 of 162\n",
      "Running combi 53 of 162\n",
      "Running combi 54 of 162\n",
      "Running combi 55 of 162\n",
      "Running combi 56 of 162\n",
      "Running combi 57 of 162\n",
      "Running combi 58 of 162\n",
      "Running combi 59 of 162\n",
      "Running combi 60 of 162\n",
      "Running combi 61 of 162\n",
      "Running combi 62 of 162\n",
      "Running combi 63 of 162\n",
      "Running combi 64 of 162\n",
      "Running combi 65 of 162\n",
      "Running combi 66 of 162\n",
      "Running combi 67 of 162\n",
      "Running combi 68 of 162\n",
      "Running combi 69 of 162\n",
      "Running combi 70 of 162\n",
      "Running combi 71 of 162\n",
      "Running combi 72 of 162\n",
      "Running combi 73 of 162\n",
      "Running combi 74 of 162\n",
      "Running combi 75 of 162\n",
      "Running combi 76 of 162\n",
      "Running combi 77 of 162\n",
      "Running combi 78 of 162\n",
      "Running combi 79 of 162\n",
      "Running combi 80 of 162\n",
      "Running combi 81 of 162\n",
      "Running combi 82 of 162\n",
      "Running combi 83 of 162\n",
      "Running combi 84 of 162\n",
      "Running combi 85 of 162\n",
      "Running combi 86 of 162\n",
      "Running combi 87 of 162\n",
      "Running combi 88 of 162\n",
      "Running combi 89 of 162\n",
      "Running combi 90 of 162\n",
      "Running combi 91 of 162\n",
      "Running combi 92 of 162\n",
      "Running combi 93 of 162\n",
      "Running combi 94 of 162\n",
      "Running combi 95 of 162\n",
      "Running combi 96 of 162\n",
      "Running combi 97 of 162\n",
      "Running combi 98 of 162\n",
      "Running combi 99 of 162\n",
      "Running combi 100 of 162\n",
      "Running combi 101 of 162\n",
      "Running combi 102 of 162\n",
      "Running combi 103 of 162\n",
      "Running combi 104 of 162\n",
      "Running combi 105 of 162\n",
      "Running combi 106 of 162\n",
      "Running combi 107 of 162\n",
      "Running combi 108 of 162\n",
      "Running combi 109 of 162\n",
      "Running combi 110 of 162\n",
      "Running combi 111 of 162\n",
      "Running combi 112 of 162\n",
      "Running combi 113 of 162\n",
      "Running combi 114 of 162\n",
      "Running combi 115 of 162\n",
      "Running combi 116 of 162\n",
      "Running combi 117 of 162\n",
      "Running combi 118 of 162\n",
      "Running combi 119 of 162\n",
      "Running combi 120 of 162\n",
      "Running combi 121 of 162\n",
      "Running combi 122 of 162\n",
      "Running combi 123 of 162\n",
      "Running combi 124 of 162\n",
      "Running combi 125 of 162\n",
      "Running combi 126 of 162\n",
      "Running combi 127 of 162\n",
      "Running combi 128 of 162\n",
      "Running combi 129 of 162\n",
      "Running combi 130 of 162\n",
      "Running combi 131 of 162\n",
      "Running combi 132 of 162\n",
      "Running combi 133 of 162\n",
      "Running combi 134 of 162\n",
      "Running combi 135 of 162\n",
      "Running combi 136 of 162\n",
      "Running combi 137 of 162\n",
      "Running combi 138 of 162\n",
      "Running combi 139 of 162\n",
      "Running combi 140 of 162\n",
      "Running combi 141 of 162\n",
      "Running combi 142 of 162\n",
      "Running combi 143 of 162\n",
      "Running combi 144 of 162\n",
      "Running combi 145 of 162\n",
      "Running combi 146 of 162\n",
      "Running combi 147 of 162\n",
      "Running combi 148 of 162\n",
      "Running combi 149 of 162\n",
      "Running combi 150 of 162\n",
      "Running combi 151 of 162\n",
      "Running combi 152 of 162\n",
      "Running combi 153 of 162\n",
      "Running combi 154 of 162\n",
      "Running combi 155 of 162\n",
      "Running combi 156 of 162\n",
      "Running combi 157 of 162\n",
      "Running combi 158 of 162\n",
      "Running combi 159 of 162\n",
      "Running combi 160 of 162\n",
      "Running combi 161 of 162\n",
      "Running combi 162 of 162\n"
     ]
    }
   ],
   "source": [
    "combi_counter = 0\n",
    "# Run all the nested combinations:\n",
    "for unique_anno in unique_anno_grid:\n",
    "    for frac_max_score in frac_max_score_grid:\n",
    "        for min_mut_freq in min_mut_freq_grid:\n",
    "            # \"iteration\" must be the innermost nest,\n",
    "            # because the result depends on the previous run:\n",
    "            for iteration in range(1, N_iterations+1):\n",
    "                combi_counter += 1\n",
    "                combi_key = '{}-{}-{}-{}'.format(unique_anno, frac_max_score, min_mut_freq, float(iteration))\n",
    "                align_res_df = pd.read_csv('align-opti_res.csv')\n",
    "                combi_key_set = {'{}-{}-{}-{}'.format(u, f, m, i) for u, f, m, i in zip(align_res_df['unique_anno'], align_res_df['frac_max_score'], align_res_df['min_mut_freq'], align_res_df['iteration'])}\n",
    "                if combi_key in combi_key_set:\n",
    "                    continue\n",
    "                print('Running combi {} of {}'.format(combi_counter, total_combi))\n",
    "\n",
    "                # At first iteration use first pass alignment,\n",
    "                # after this use the masked alignments:\n",
    "                if iteration == 1:\n",
    "                    dir_dict_iter = dir_dict\n",
    "                else:\n",
    "                    dir_dict_iter = dir_dict_masked\n",
    "                # Perform transcript mutation analysis:\n",
    "                TM_obj = TM_analysis(dir_dict_iter, sample_df, tRNA_database, \\\n",
    "                                     common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                TM_obj.find_muts(n_jobs=12, unique_anno=unique_anno, verbose=False)\n",
    "                TM_obj.mask_tRNA_database(min_mut_freq=min_mut_freq, frac_max_score=frac_max_score, \\\n",
    "                                          min_pos_count=100, min_tr_count=200)\n",
    "                tRNA_database_masked = TM_obj.write_masked_tRNA_database(out_dir='tRNA_database_masked')\n",
    "\n",
    "                # Run alignment:\n",
    "                align_obj = SWIPE_align(dir_dict_masked, tRNA_database_masked, sample_df, SWIPE_score_mat2, \\\n",
    "                                        gap_penalty=6, extension_penalty=3, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                                        common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                sample_df = align_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                # Collect alignment statistics:\n",
    "                stats_obj = STATS_collection(dir_dict_masked, tRNA_data, sample_df, \\\n",
    "                                             common_seqs=common_seqs, overwrite_dir=True)\n",
    "                stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                # Collect alignment stats and append to file:\n",
    "                col_sele = ['sample_name_unique', 'Mapping_percent', \\\n",
    "                            'percent_single_annotation', 'percent_multiple_codons']\n",
    "                align_res = sample_df.loc[:, col_sele]\n",
    "                align_res['unique_anno'] = unique_anno\n",
    "                align_res['frac_max_score'] = frac_max_score\n",
    "                align_res['min_mut_freq'] = min_mut_freq\n",
    "                align_res['iteration'] = iteration\n",
    "                with open('align-opti_res.csv', 'a') as fh_res:\n",
    "                    align_res.to_csv(fh_res, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
