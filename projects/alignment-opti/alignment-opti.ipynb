{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, shutil, bz2, copy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import numpy as np\n",
    "\n",
    "### Plotting imports ###\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import StepPatch\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "import logomaker as lm\n",
    "palette = list(mcolors.TABLEAU_COLORS.keys())\n",
    "sns.set_theme(style=\"ticks\", palette=\"muted\")\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is in: /Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq/projects/alignment-opti\n",
      "Repo is in: /Users/krdav/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq\n",
      "Using minimum read length: 39 (after merge)\n",
      "Using minimum alignemnt score: 15\n"
     ]
    }
   ],
   "source": [
    "# Navigate back to NBdir in case of re-running a code block:\n",
    "if not 'NBdir' in globals():\n",
    "    NBdir = os.getcwd()\n",
    "print('Notebook is in: {}'.format(NBdir))\n",
    "os.chdir(NBdir)  # If you changed the current working dir, this will take you back to the notebook dir.\n",
    "\n",
    "# Define the path to the repo folder.\n",
    "# Change if necessary.\n",
    "homedir = '/'.join(NBdir.split('/')[0:-2])\n",
    "print('Repo is in: {}'.format(homedir))\n",
    "sys.path.insert(1, homedir)\n",
    "from src.misc import index_to_sample_df, downsample_raw_input, read_tRNAdb_info, sample_df_to_dict\n",
    "from src.read_processing import AR_merge, BC_split, Kmer_analysis, BC_analysis, UMI_trim\n",
    "from src.alignment import SWIPE_align\n",
    "from src.stats_collection import STATS_collection\n",
    "from src.plotting import TRNA_plot\n",
    "from src.transcript_mutations import TM_analysis\n",
    "\n",
    "# These are default folder names for data and raw fastq files\n",
    "# relative to the folder in which this notebook is in:\n",
    "data_dir = 'data'\n",
    "seq_dir = 'raw_fastq'\n",
    "seq_dir_noDS = seq_dir # Not downsampled\n",
    "\n",
    "# These folder names are used in subsequent processing steps\n",
    "# to dump data. Best to not change:\n",
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "BC_dir = 'BC_split'\n",
    "UMI_dir = 'UMI_trimmed'\n",
    "align_dir = 'SWalign'\n",
    "stats_dir = 'stats_collection'\n",
    "TM_dir = 'transcript_mutations'\n",
    "plotting_dir = 'plotting'\n",
    "tRNA_database = dict()\n",
    "tRNA_database['human'] = '{}/tRNA_database/human/hg38-tRNAs.fa'.format(homedir)\n",
    "tRNA_database['mouse'] = '{}/tRNA_database/mouse/mm10-tRNAs.fa'.format(homedir)\n",
    "# Read information (length, codon etc) of tRNAs into dictionary:\n",
    "tRNA_data = read_tRNAdb_info(tRNA_database)\n",
    "SWIPE_score_mat = '{}/utils/nuc_score-matrix.txt'.format(homedir)\n",
    "SWIPE_score_mat2 = '{}/utils/nuc_score-matrix_2.txt'.format(homedir) # For masked reference sequences\n",
    "# tRNA sequencing yields many duplicated reads.\n",
    "# Adding these commonly seen sequences to a list prevents duplicated alignment:\n",
    "common_seqs = '{}/utils/common-seqs.fasta.bz2'.format(homedir)\n",
    "# ^^ That one is too big, take the top 1k instead:\n",
    "common_seqs = '{}/utils/common-seqs_1k.fasta.bz2'.format(homedir)\n",
    "\n",
    "# Define minimum read length based on minimum insert size:\n",
    "MIN_INSERT_LEN = 10\n",
    "UMI_LEN = 10\n",
    "BC_MAX_LEN = 19\n",
    "MIN_READ_LEN = MIN_INSERT_LEN + UMI_LEN + BC_MAX_LEN\n",
    "print('Using minimum read length: {} (after merge)'.format(MIN_READ_LEN))\n",
    "\n",
    "# The minimum alignment score.\n",
    "# Better to set relatively low, since additional filtering can\n",
    "# be applied later.\n",
    "MIN_SCORE_ALIGN = 15\n",
    "print('Using minimum alignemnt score: {}'.format(MIN_SCORE_ALIGN))\n",
    "\n",
    "# Read index information:\n",
    "index_list_fnam = 'index_list.xlsx'\n",
    "index_df = pd.read_excel('{}/utils/{}'.format(homedir, index_list_fnam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_list_fnam = 'sample_list_alignment-opti.xlsx'\n",
    "sample_list_fnam = 'sample_list_alignment-opti_DS.xlsx'\n",
    "sample_df = pd.read_excel('{}/{}'.format(NBdir, sample_list_fnam))\n",
    "# Add barcode sequences:\n",
    "sample_df = index_to_sample_df(sample_df, index_df)\n",
    "# Read elementary info (replicate, barcode, species)\n",
    "# for each unique sample name into a dictionary:\n",
    "sample_dict = sample_df_to_dict(sample_df)\n",
    "# Get filenames from the sample information:\n",
    "inp_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].copy().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Make a dictionary with paths used for data processing:\n",
    "dir_dict = dict(NBdir = NBdir,\n",
    "                data_dir = data_dir,\n",
    "                seq_dir = seq_dir,\n",
    "                AdapterRemoval_dir = AdapterRemoval_dir,\n",
    "                BC_dir = BC_dir,\n",
    "                UMI_dir = UMI_dir,\n",
    "                align_dir = align_dir,\n",
    "                stats_dir = stats_dir,\n",
    "                TM_dir = TM_dir,\n",
    "                plotting_dir = plotting_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using common sequences to prevent duplicated alignment.\n",
      "Running Swipe on:  1h_1  32m_1  8m_1  0m_1  40h_1  8h_1  40h_NoOx_1  16h_1  4h_1  common-seqs\n",
      "Collecting alignment statistics, from sample:  8m_1  0m_1  32m_1  1h_1  40h_1  8h_1  40h_NoOx_1  16h_1  4h_1  common-seqsUsing common sequences...\n",
      "Collecting stats from:  0m_1  32m_1  8m_1  1h_1  40h_1  8h_1  40h_NoOx_1  16h_1  4h_1"
     ]
    }
   ],
   "source": [
    "# Run AdapterRemoval:\n",
    "AR_obj = AR_merge(dir_dict, inp_file_df, MIN_READ_LEN, overwrite_dir=True)\n",
    "inp_file_df = AR_obj.run_parallel(n_jobs=4)\n",
    "\n",
    "# Split files based on barcodes:\n",
    "BCsplit_obj = BC_split(dir_dict, sample_df, inp_file_df, overwrite_dir=True)\n",
    "sample_df, inp_file_df = BCsplit_obj.run_parallel(n_jobs=4)\n",
    "\n",
    "# Trim UMI:\n",
    "UMItrim_obj = UMI_trim(dir_dict, sample_df, overwrite_dir=True)\n",
    "sample_df = UMItrim_obj.run_parallel(n_jobs=4)\n",
    "\n",
    "# First pass alignment:\n",
    "align_obj = SWIPE_align(dir_dict, tRNA_database, sample_df, SWIPE_score_mat, \\\n",
    "                        gap_penalty=6, extension_penalty=1, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                        common_seqs=common_seqs, overwrite_dir=True)\n",
    "sample_df = align_obj.run_parallel(n_jobs=4, verbose=False)\n",
    "\n",
    "# Collect alignment statistics:\n",
    "stats_obj = STATS_collection(dir_dict, tRNA_data, sample_df, common_seqs=common_seqs, overwrite_dir=True)\n",
    "stats_df = stats_obj.run_parallel(n_jobs=4, verbose=False)\n",
    "\n",
    "# Write first alignment stats:\n",
    "align_res = sample_df.loc[:, ['sample_name_unique', 'Mapping_percent', 'percent_single_annotation']]\n",
    "align_res['unique_anno'] = None\n",
    "align_res['frac_max_score'] = None\n",
    "align_res['min_mut_freq'] = None\n",
    "align_res['iteration'] = None\n",
    "with open('align-opti_res.csv', 'w') as fh_res:\n",
    "    align_res.to_csv(fh_res, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_dict_masked = copy.deepcopy(dir_dict)\n",
    "dir_dict_masked['align_dir'] = 'SWalign_masked'\n",
    "dir_dict_masked['stats_dir'] = 'stats_collection_masked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of variables to test:\n",
    "min_mut_freq_grid = [0.4, 0.6, 0.7, 0.75, 0.8, \\\n",
    "                     0.83, 0.86, 0.89, 0.95]\n",
    "frac_max_score_grid = [0.90, 0.95, 1]\n",
    "unique_anno_grid = [True, False]\n",
    "N_iterations = 3\n",
    "total_combi = len(min_mut_freq_grid) * len(frac_max_score_grid) * len(unique_anno_grid) * N_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combi 1 of 162\n",
      "Using common sequences...\n",
      "Using common sequences to prevent duplicated alignment.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mException\u001b[0m: \n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-441bf07716da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                         \u001b[0mgap_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension_penalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_score_align\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMIN_SCORE_ALIGN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                         common_seqs=common_seqs, overwrite_dir=True)\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# Collect alignment statistics:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/MCB/Sullivan_lab/tRNA-charge-seq/src/alignment.py\u001b[0m in \u001b[0;36mrun_parallel\u001b[0;34m(self, n_jobs, overwrite, verbose, load_previous)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nCollecting alignment statistics, from sample:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mWorkerPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__collect_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__write_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mpire/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable_of_args, iterable_len, max_tasks_active, chunk_size, n_splits, worker_lifespan, progress_bar, progress_bar_position, concatenate_numpy_output, worker_init, worker_exit, task_timeout, worker_init_timeout, worker_exit_timeout, progress_bar_options)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miterable_len\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_of_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0miterable_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_of_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         results = self.map_unordered(func, ((args_idx, args) for args_idx, args in enumerate(iterable_of_args)),\n\u001b[0m\u001b[1;32m    371\u001b[0m                                      \u001b[0miterable_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tasks_active\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_lifespan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                                      \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_exit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mpire/pool.py\u001b[0m in \u001b[0;36mmap_unordered\u001b[0;34m(self, func, iterable_of_args, iterable_len, max_tasks_active, chunk_size, n_splits, worker_lifespan, progress_bar, progress_bar_position, worker_init, worker_exit, task_timeout, worker_init_timeout, worker_exit_timeout, progress_bar_options)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# Simply call imap and cast it to a list. This make sure all elements are there before returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         return list(self.imap_unordered(func, iterable_of_args, iterable_len, max_tasks_active, chunk_size,\n\u001b[0m\u001b[1;32m    440\u001b[0m                                         \u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_lifespan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                                         \u001b[0mworker_exit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_init_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_exit_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mpire/pool.py\u001b[0m in \u001b[0;36mimap_unordered\u001b[0;34m(self, func, iterable_of_args, iterable_len, max_tasks_active, chunk_size, n_splits, worker_lifespan, progress_bar, progress_bar_position, worker_init, worker_exit, task_timeout, worker_init_timeout, worker_exit_timeout, progress_bar_options)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0;31m# Join exception queue, if it hasn't already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mpire/pool.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, progress_bar_handler)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mtraceback_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhighlight_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising obtained exception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraceback_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     def stop_and_join(self, progress_bar_handler: Optional[ProgressBarHandler] = None,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "combi_counter = 0\n",
    "# Run all the nested combinations:\n",
    "for unique_anno in unique_anno_grid:\n",
    "    for frac_max_score in frac_max_score_grid:\n",
    "        for min_mut_freq in min_mut_freq_grid:\n",
    "            # \"iteration\" must be the innermost nest,\n",
    "            # because the result depends on the previous run:\n",
    "            for iteration in range(1, N_iterations+1):\n",
    "                combi_counter += 1\n",
    "                print('Running combi {} of {}'.format(combi_counter, total_combi))\n",
    "\n",
    "                # At first iteration use first pass alignment,\n",
    "                # after this use the masked alignments:\n",
    "                if iteration == 1:\n",
    "                    dir_dict_iter = dir_dict\n",
    "                else:\n",
    "                    dir_dict_iter = dir_dict_masked\n",
    "                # Perform transcript mutation analysis:\n",
    "                TM_obj = TM_analysis(dir_dict_iter, sample_df, tRNA_database, \\\n",
    "                                     common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                TM_obj.find_muts(n_jobs=4, unique_anno=unique_anno, verbose=False)\n",
    "                TM_obj.mask_tRNA_database(min_mut_freq=min_mut_freq, frac_max_score=frac_max_score, \\\n",
    "                                          min_pos_count=100, min_tr_count=200)\n",
    "                tRNA_database_masked = TM_obj.write_masked_tRNA_database(out_dir='tRNA_database_masked')\n",
    "\n",
    "                # Run alignment:\n",
    "                align_obj = SWIPE_align(dir_dict_masked, tRNA_database_masked, sample_df, SWIPE_score_mat2, \\\n",
    "                                        gap_penalty=6, extension_penalty=3, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                                        common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                sample_df = align_obj.run_parallel(n_jobs=4, verbose=False)\n",
    "\n",
    "                # Collect alignment statistics:\n",
    "                stats_obj = STATS_collection(dir_dict_masked, tRNA_data, sample_df, \\\n",
    "                                             common_seqs=common_seqs, overwrite_dir=True)\n",
    "                stats_df = stats_obj.run_parallel(n_jobs=4, verbose=False)\n",
    "\n",
    "                # Collect alignment stats and append to file:\n",
    "                col_sele = ['sample_name_unique', 'Mapping_percent', 'percent_single_annotation']\n",
    "                align_res = sample_df.loc[:, col_sele]\n",
    "                align_res['unique_anno'] = unique_anno\n",
    "                align_res['frac_max_score'] = frac_max_score\n",
    "                align_res['min_mut_freq'] = min_mut_freq\n",
    "                align_res['iteration'] = iteration\n",
    "                with open('align-opti_res.csv', 'a') as fh_res:\n",
    "                    align_res.to_csv(fh_res, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
