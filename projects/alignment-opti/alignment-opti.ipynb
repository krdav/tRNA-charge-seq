{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, shutil, bz2, copy, warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "# Numpy will complain about subnormals if python\n",
    "# is compiled with the -ffast-math compiler flag:\n",
    "# https://github.com/clearlinux/distribution/issues/2809\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"The value of the smallest subnormal for\")\n",
    "    import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is in: /home/sulab/tRNA-charge-seq/projects/alignment-opti\n",
      "Repo is in: /home/sulab/tRNA-charge-seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using minimum read length: 39 (after merge)\n",
      "Using minimum alignemnt score: 15\n"
     ]
    }
   ],
   "source": [
    "# Navigate back to NBdir in case of re-running a code block:\n",
    "if not 'NBdir' in globals():\n",
    "    NBdir = os.getcwd()\n",
    "print('Notebook is in: {}'.format(NBdir))\n",
    "os.chdir(NBdir)  # If you changed the current working dir, this will take you back to the notebook dir.\n",
    "\n",
    "# Define the path to the repo folder.\n",
    "# Change if necessary.\n",
    "homedir = '/'.join(NBdir.split('/')[0:-2])\n",
    "print('Repo is in: {}'.format(homedir))\n",
    "sys.path.insert(1, homedir)\n",
    "from src.misc import index_to_sample_df, downsample_raw_input, read_tRNAdb_info, sample_df_to_dict\n",
    "from src.read_processing import AR_merge, BC_split, Kmer_analysis, BC_analysis, UMI_trim\n",
    "from src.alignment import SWIPE_align\n",
    "from src.stats_collection import STATS_collection\n",
    "from src.plotting import TRNA_plot\n",
    "from src.transcript_mutations import TM_analysis\n",
    "\n",
    "# These are default folder names for data and raw fastq files\n",
    "# relative to the folder in which this notebook is in:\n",
    "data_dir = 'data'\n",
    "seq_dir = 'raw_fastq'\n",
    "seq_dir_noDS = seq_dir # Not downsampled\n",
    "\n",
    "# These folder names are used in subsequent processing steps\n",
    "# to dump data. Best to not change:\n",
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "BC_dir = 'BC_split'\n",
    "UMI_dir = 'UMI_trimmed'\n",
    "align_dir = 'SWalign'\n",
    "stats_dir = 'stats_collection'\n",
    "TM_dir = 'transcript_mutations'\n",
    "plotting_dir = 'plotting'\n",
    "tRNA_database = dict()\n",
    "tRNA_database['human'] = '{}/tRNA_database/human/hg38-tRNAs.fa'.format(homedir)\n",
    "tRNA_database['mouse'] = '{}/tRNA_database/mouse/mm10-tRNAs.fa'.format(homedir)\n",
    "# Read information (length, codon etc) of tRNAs into dictionary:\n",
    "tRNA_data = read_tRNAdb_info(tRNA_database)\n",
    "SWIPE_score_mat = '{}/utils/nuc_score-matrix.txt'.format(homedir)\n",
    "SWIPE_score_mat2 = '{}/utils/nuc_score-matrix_2.txt'.format(homedir) # For masked reference sequences\n",
    "# tRNA sequencing yields many duplicated reads.\n",
    "# Adding these commonly seen sequences to a list prevents duplicated alignment:\n",
    "common_seqs = '{}/utils/common-seqs.fasta.bz2'.format(homedir)\n",
    "\n",
    "# Define minimum read length based on minimum insert size:\n",
    "MIN_INSERT_LEN = 10\n",
    "UMI_LEN = 10\n",
    "BC_MAX_LEN = 19\n",
    "MIN_READ_LEN = MIN_INSERT_LEN + UMI_LEN + BC_MAX_LEN\n",
    "print('Using minimum read length: {} (after merge)'.format(MIN_READ_LEN))\n",
    "\n",
    "# The minimum alignment score.\n",
    "# Better to set relatively low, since additional filtering can\n",
    "# be applied later.\n",
    "MIN_SCORE_ALIGN = 15\n",
    "print('Using minimum alignemnt score: {}'.format(MIN_SCORE_ALIGN))\n",
    "\n",
    "# Read index information:\n",
    "index_list_fnam = 'index_list.xlsx'\n",
    "index_df = pd.read_excel('{}/utils/{}'.format(homedir, index_list_fnam))\n",
    "\n",
    "# Read sample list:\n",
    "sample_list_fnam = 'sample_list_alignment-opti.xlsx'\n",
    "sample_df = pd.read_excel('{}/{}'.format(NBdir, sample_list_fnam))\n",
    "# Add barcode sequences:\n",
    "sample_df = index_to_sample_df(sample_df, index_df)\n",
    "# Read elementary info (replicate, barcode, species)\n",
    "# for each unique sample name into a dictionary:\n",
    "sample_dict = sample_df_to_dict(sample_df)\n",
    "# Get filenames from the sample information:\n",
    "inp_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].copy().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Make a dictionary with paths used for data processing:\n",
    "dir_dict = dict(NBdir = NBdir,\n",
    "                data_dir = data_dir,\n",
    "                seq_dir = seq_dir,\n",
    "                AdapterRemoval_dir = AdapterRemoval_dir,\n",
    "                BC_dir = BC_dir,\n",
    "                UMI_dir = UMI_dir,\n",
    "                align_dir = align_dir,\n",
    "                stats_dir = stats_dir,\n",
    "                TM_dir = TM_dir,\n",
    "                plotting_dir = plotting_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment optimization grid search\n",
    "\n",
    "tRNAs are heavily modified post-transcriptionally and some of these modifications induce skipping or wrong base incorporation during the reverse transcription step.\n",
    "At the level of alignment, RT skipping will appear as gaps and wrong base incorporation will appear as mismatches, both leading to lower alignment scores.\n",
    "The non-random nature of such modifications is problematic because it can lead to non-random misannotation by flipping the alignment score in favor of a closely related, but wrong, annotation.\n",
    "\n",
    "There are three strategies to increase alignment specificity: \n",
    "1. Increase alignment length i.e. decrease RT fall-off\n",
    "2. Remove tRNA modification i.e. demodify tRNA before the RT step\n",
    "3. Adjust the alignment to accomodate RT artifacts\n",
    "\n",
    "Here, we are exploring the third option by applying \"N\" masking of positions in the reference sequences that are highly modified.\n",
    "N masking causes the masked positions to have no contribution to the alignment score.\n",
    "Therefore it is a very unspecific approach, as opposed to a position specific scoring matrix (PSSM) or a hidden Markov model (HMM) that would score the types of mismatches differently.\n",
    "The advantage of the unspecific approach is that fewer assumptions are made.\n",
    "Using a PSSM or HMM would require inferring these models on a dataset of tRNA reads, with the implicit assumption that each read is drawn randomly from the distribution of all possible reads.\n",
    "That assumption is quite obviously wrong, given that the penetrance of a tRNA modification can change as a response to a biological event.\n",
    "\n",
    "In the following, reference sequence masking is made using the mismatches observed in the alignment i.e. if the tRNA reads have a high degree of mismatches on a certain position in the reference, then this position in the reference is masked.\n",
    "The masking will alter the alignment and thus iterations can be performed using the new alignment to make a new masked reference etc.\n",
    "The masking has four tuning parameters:\n",
    "1. \"min_mut_freq\": The minimum mismatch frequency to trigger masking\n",
    "2. \"unique_anno\": Use only uniquely annotated reads for masking (True/False)\n",
    "3. \"frac_max_score\": The minimum fraction of the maximum alignment score between two reference sequences to expand the masked positions in one reference to another\n",
    "4. \"iteration\": The number of iterations performed\n",
    "\n",
    "The purpose of the grid search is to find the best set of tuning parameters and use these in future alignments.\n",
    "A first pass alignment has to be generated, then the grid search is performed and finally the best performing masked reference sequences are extracted for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing folder because overwrite set to false: /home/sulab/tRNA-charge-seq/projects/alignment-opti/data/AdapterRemoval\n",
      "Using existing folder because overwrite set to false: /home/sulab/tRNA-charge-seq/projects/alignment-opti/data/BC_split\n",
      "Loaded results from previous run... Not running barcode split.\n",
      "Using existing folder because overwrite set to false: /home/sulab/tRNA-charge-seq/projects/alignment-opti/data/UMI_trimmed\n",
      "Loaded results from previous run... Not running UMI trimming.\n",
      "Using existing folder because overwrite set to false: /home/sulab/tRNA-charge-seq/projects/alignment-opti/data/SWalign\n",
      "Loaded results from previous run... Not running alignment.\n",
      "Using existing folder because overwrite set to false: /home/sulab/tRNA-charge-seq/projects/alignment-opti/data/stats_collection\n",
      "Loaded results from previous run... Not running stats collection.\n"
     ]
    }
   ],
   "source": [
    "# If restarting after shutdown,\n",
    "# no need to re-run first pass alignment:\n",
    "if False:\n",
    "    # Run AdapterRemoval:\n",
    "    AR_obj = AR_merge(dir_dict, inp_file_df, MIN_READ_LEN, overwrite_dir=True)\n",
    "    inp_file_df = AR_obj.run_parallel(n_jobs=4)\n",
    "\n",
    "    # Split files based on barcodes:\n",
    "    BCsplit_obj = BC_split(dir_dict, sample_df, inp_file_df, overwrite_dir=True)\n",
    "    sample_df, inp_file_df = BCsplit_obj.run_parallel(n_jobs=12)\n",
    "\n",
    "    # Trim UMI:\n",
    "    UMItrim_obj = UMI_trim(dir_dict, sample_df, overwrite_dir=True)\n",
    "    sample_df = UMItrim_obj.run_parallel(n_jobs=12)\n",
    "\n",
    "    # First pass alignment:\n",
    "    align_obj = SWIPE_align(dir_dict, tRNA_database, sample_df, SWIPE_score_mat, \\\n",
    "                            gap_penalty=6, extension_penalty=1, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                            common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "    sample_df = align_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "    # Collect alignment statistics:\n",
    "    stats_obj = STATS_collection(dir_dict, tRNA_data, sample_df, common_seqs=common_seqs, \\\n",
    "                                 overwrite_dir=True)\n",
    "    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "    # Write first alignment stats:\n",
    "    align_res = sample_df.loc[:, ['sample_name_unique', 'Mapping_percent', \\\n",
    "                                  'percent_single_annotation', 'percent_multiple_codons']]\n",
    "    align_res['unique_anno'] = None\n",
    "    align_res['frac_max_score'] = None\n",
    "    align_res['min_mut_freq'] = None\n",
    "    align_res['iteration'] = None\n",
    "    with open('align-opti_res.csv', 'w') as fh_res:\n",
    "        align_res.to_csv(fh_res, index=False)\n",
    "\n",
    "else:\n",
    "    # Run AdapterRemoval:\n",
    "    AR_obj = AR_merge(dir_dict, inp_file_df, MIN_READ_LEN, overwrite_dir=False)\n",
    "    inp_file_df = AR_obj.run_parallel(n_jobs=4, overwrite=False)\n",
    "\n",
    "    # Split files based on barcodes:\n",
    "    BCsplit_obj = BC_split(dir_dict, sample_df, inp_file_df, overwrite_dir=False)\n",
    "    sample_df, inp_file_df = BCsplit_obj.run_parallel(n_jobs=12, load_previous=True)\n",
    "\n",
    "    # Trim UMI:\n",
    "    UMItrim_obj = UMI_trim(dir_dict, sample_df, overwrite_dir=False)\n",
    "    sample_df = UMItrim_obj.run_parallel(n_jobs=12, load_previous=True)\n",
    "\n",
    "    # First pass alignment:\n",
    "    align_obj = SWIPE_align(dir_dict, tRNA_database, sample_df, SWIPE_score_mat, \\\n",
    "                            gap_penalty=6, extension_penalty=1, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                            common_seqs=common_seqs, overwrite_dir=False, verbose=False)\n",
    "    sample_df = align_obj.run_parallel(n_jobs=12, verbose=False, load_previous=True)\n",
    "\n",
    "    # Collect alignment statistics:\n",
    "    stats_obj = STATS_collection(dir_dict, tRNA_data, sample_df, common_seqs=common_seqs, \\\n",
    "                                 overwrite_dir=False)\n",
    "    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False, load_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a grid of alignment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_dict_masked = copy.deepcopy(dir_dict)\n",
    "dir_dict_masked['align_dir'] = 'SWalign_masked'\n",
    "dir_dict_masked['stats_dir'] = 'stats_collection_masked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of variables to test:\n",
    "min_mut_freq_grid = [0.5, 0.7, 0.75, 0.78, 0.8, \\\n",
    "                     0.82, 0.84, 0.86, 0.9]\n",
    "frac_max_score_grid = [0.90, 0.95, 1]\n",
    "unique_anno_grid = [True, False]\n",
    "N_iterations = 3\n",
    "total_combi = len(min_mut_freq_grid) * len(frac_max_score_grid) * len(unique_anno_grid) * N_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Switch on to do the search\n",
    "    combi_counter = 0\n",
    "    # Run all the nested combinations:\n",
    "    for unique_anno in unique_anno_grid:\n",
    "        for frac_max_score in frac_max_score_grid:\n",
    "            for min_mut_freq in min_mut_freq_grid:\n",
    "                # \"iteration\" must be the innermost nest,\n",
    "                # because the result depends on the previous run:\n",
    "                for iteration in range(1, N_iterations+1):\n",
    "                    combi_counter += 1\n",
    "                    combi_key = '{}-{}-{}-{}'.format(unique_anno, frac_max_score, min_mut_freq, float(iteration))\n",
    "                    align_res_df = pd.read_csv('align-opti_res.csv')\n",
    "                    combi_key_set = {'{}-{}-{}-{}'.format(u, f, m, i) for u, f, m, i in zip(align_res_df['unique_anno'], align_res_df['frac_max_score'], align_res_df['min_mut_freq'], align_res_df['iteration'])}\n",
    "                    if combi_key in combi_key_set:\n",
    "                        continue\n",
    "                    print('Running combi {} of {}'.format(combi_counter, total_combi))\n",
    "\n",
    "                    # At first iteration use first pass alignment,\n",
    "                    # after this use the masked alignments:\n",
    "                    if iteration == 1:\n",
    "                        dir_dict_iter = dir_dict\n",
    "                    else:\n",
    "                        dir_dict_iter = dir_dict_masked\n",
    "                    # Perform transcript mutation analysis:\n",
    "                    TM_obj = TM_analysis(dir_dict_iter, sample_df, tRNA_database, \\\n",
    "                                         common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                    TM_obj.find_muts(n_jobs=12, unique_anno=unique_anno, verbose=False)\n",
    "                    TM_obj.mask_tRNA_database(min_mut_freq=min_mut_freq, frac_max_score=frac_max_score, \\\n",
    "                                              min_pos_count=100, min_tr_count=200)\n",
    "                    tRNA_database_masked = TM_obj.write_masked_tRNA_database(out_dir='tRNA_database_masked')\n",
    "\n",
    "                    # Run alignment:\n",
    "                    align_obj = SWIPE_align(dir_dict_masked, tRNA_database_masked, sample_df, SWIPE_score_mat2, \\\n",
    "                                            gap_penalty=6, extension_penalty=3, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                                            common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                    sample_df = align_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                    # Collect alignment statistics:\n",
    "                    stats_obj = STATS_collection(dir_dict_masked, tRNA_data, sample_df, \\\n",
    "                                                 common_seqs=common_seqs, overwrite_dir=True)\n",
    "                    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                    # Collect alignment stats and append to file:\n",
    "                    col_sele = ['sample_name_unique', 'Mapping_percent', \\\n",
    "                                'percent_single_annotation', 'percent_multiple_codons']\n",
    "                    align_res = sample_df.loc[:, col_sele]\n",
    "                    align_res['unique_anno'] = unique_anno\n",
    "                    align_res['frac_max_score'] = frac_max_score\n",
    "                    align_res['min_mut_freq'] = min_mut_freq\n",
    "                    align_res['iteration'] = iteration\n",
    "                    with open('align-opti_res.csv', 'a') as fh_res:\n",
    "                        align_res.to_csv(fh_res, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make final masked database from best alignment parameters\n",
    "The best parameters were found in the data processing and plotting notebook: `alignment-opti_plotting.ipynb`\n",
    "The resulting masked reference sequences folder is moved out of this projects folder and into the main repo.\n",
    "Future alignments can then use these masked references for alignment directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters:\n",
    "min_mut_freq_grid = [0.84]\n",
    "frac_max_score_grid = [0.95]\n",
    "unique_anno_grid = [True]\n",
    "N_iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat of the grid search above.\n",
    "# This is done so because 3 iterations\n",
    "# have to be performed.\n",
    "combi_counter = 0\n",
    "# Run all the nested combinations:\n",
    "for unique_anno in unique_anno_grid:\n",
    "    for frac_max_score in frac_max_score_grid:\n",
    "        for min_mut_freq in min_mut_freq_grid:\n",
    "            # \"iteration\" must be the innermost nest,\n",
    "            # because the result depends on the previous run:\n",
    "            for iteration in range(1, N_iterations+1):\n",
    "                # At first iteration use first pass alignment,\n",
    "                # after this use the masked alignments:\n",
    "                if iteration == 1:\n",
    "                    dir_dict_iter = dir_dict\n",
    "                else:\n",
    "                    dir_dict_iter = dir_dict_masked\n",
    "                # Perform transcript mutation analysis:\n",
    "                TM_obj = TM_analysis(dir_dict_iter, sample_df, tRNA_database, \\\n",
    "                                     common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                TM_obj.find_muts(n_jobs=12, unique_anno=unique_anno, verbose=False)\n",
    "                TM_obj.mask_tRNA_database(min_mut_freq=min_mut_freq, frac_max_score=frac_max_score, \\\n",
    "                                          min_pos_count=100, min_tr_count=200)\n",
    "                tRNA_database_masked = TM_obj.write_masked_tRNA_database(out_dir='tRNA_database_masked')\n",
    "\n",
    "                # Run alignment:\n",
    "                align_obj = SWIPE_align(dir_dict_masked, tRNA_database_masked, sample_df, SWIPE_score_mat2, \\\n",
    "                                        gap_penalty=6, extension_penalty=3, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                                        common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                sample_df = align_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                # Collect alignment statistics:\n",
    "                stats_obj = STATS_collection(dir_dict_masked, tRNA_data, sample_df, \\\n",
    "                                             common_seqs=common_seqs, overwrite_dir=True)\n",
    "                stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                # Collect alignment stats and append to file:\n",
    "                col_sele = ['sample_name_unique', 'Mapping_percent', \\\n",
    "                            'percent_single_annotation', 'percent_multiple_codons']\n",
    "                align_res = sample_df.loc[:, col_sele]\n",
    "                align_res['unique_anno'] = unique_anno\n",
    "                align_res['frac_max_score'] = frac_max_score\n",
    "                align_res['min_mut_freq'] = min_mut_freq\n",
    "                align_res['iteration'] = iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name_unique</th>\n",
       "      <th>Mapping_percent</th>\n",
       "      <th>percent_single_annotation</th>\n",
       "      <th>percent_multiple_codons</th>\n",
       "      <th>unique_anno</th>\n",
       "      <th>frac_max_score</th>\n",
       "      <th>min_mut_freq</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0m_1</td>\n",
       "      <td>99.254981</td>\n",
       "      <td>76.690905</td>\n",
       "      <td>3.907473</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8m_1</td>\n",
       "      <td>98.802818</td>\n",
       "      <td>75.817859</td>\n",
       "      <td>4.984633</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32m_1</td>\n",
       "      <td>99.305103</td>\n",
       "      <td>76.592165</td>\n",
       "      <td>3.938176</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1h_1</td>\n",
       "      <td>99.108404</td>\n",
       "      <td>76.036195</td>\n",
       "      <td>5.204059</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4h_1</td>\n",
       "      <td>98.659258</td>\n",
       "      <td>74.307626</td>\n",
       "      <td>5.219481</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8h_1</td>\n",
       "      <td>99.279907</td>\n",
       "      <td>76.708604</td>\n",
       "      <td>3.596484</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16h_1</td>\n",
       "      <td>99.057393</td>\n",
       "      <td>76.365604</td>\n",
       "      <td>4.594382</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40h_1</td>\n",
       "      <td>98.808977</td>\n",
       "      <td>76.318785</td>\n",
       "      <td>4.307895</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40h_NoOx_1</td>\n",
       "      <td>98.779005</td>\n",
       "      <td>76.238810</td>\n",
       "      <td>3.609075</td>\n",
       "      <td>True</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_name_unique  Mapping_percent  percent_single_annotation  \\\n",
       "0               0m_1        99.254981                  76.690905   \n",
       "1               8m_1        98.802818                  75.817859   \n",
       "2              32m_1        99.305103                  76.592165   \n",
       "3               1h_1        99.108404                  76.036195   \n",
       "4               4h_1        98.659258                  74.307626   \n",
       "5               8h_1        99.279907                  76.708604   \n",
       "6              16h_1        99.057393                  76.365604   \n",
       "7              40h_1        98.808977                  76.318785   \n",
       "8         40h_NoOx_1        98.779005                  76.238810   \n",
       "\n",
       "   percent_multiple_codons  unique_anno  frac_max_score  min_mut_freq  \\\n",
       "0                 3.907473         True            0.95          0.84   \n",
       "1                 4.984633         True            0.95          0.84   \n",
       "2                 3.938176         True            0.95          0.84   \n",
       "3                 5.204059         True            0.95          0.84   \n",
       "4                 5.219481         True            0.95          0.84   \n",
       "5                 3.596484         True            0.95          0.84   \n",
       "6                 4.594382         True            0.95          0.84   \n",
       "7                 4.307895         True            0.95          0.84   \n",
       "8                 3.609075         True            0.95          0.84   \n",
       "\n",
       "   iteration  \n",
       "0          3  \n",
       "1          3  \n",
       "2          3  \n",
       "3          3  \n",
       "4          3  \n",
       "5          3  \n",
       "6          3  \n",
       "7          3  \n",
       "8          3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mapping_percent              99.006205\n",
       "percent_single_annotation    76.119617\n",
       "percent_multiple_codons       4.373517\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_res.loc[:, ['Mapping_percent', 'percent_single_annotation', 'percent_multiple_codons']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
