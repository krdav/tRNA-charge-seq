{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, shutil, bz2, copy, warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "# Numpy will complain about subnormals if python\n",
    "# is compiled with the -ffast-math compiler flag:\n",
    "# https://github.com/clearlinux/distribution/issues/2809\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"The value of the smallest subnormal for\")\n",
    "    import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is in: /home/sulab/tRNA-charge-seq/projects/alignment-opti\n",
      "Repo is in: /home/sulab/tRNA-charge-seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/sulab/anaconda3/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using minimum read length: 39 (after merge)\n",
      "Using minimum alignemnt score: 15\n"
     ]
    }
   ],
   "source": [
    "# Navigate back to NBdir in case of re-running a code block:\n",
    "if not 'NBdir' in globals():\n",
    "    NBdir = os.getcwd()\n",
    "print('Notebook is in: {}'.format(NBdir))\n",
    "os.chdir(NBdir)  # If you changed the current working dir, this will take you back to the notebook dir.\n",
    "\n",
    "# Define the path to the repo folder.\n",
    "# Change if necessary.\n",
    "homedir = '/'.join(NBdir.split('/')[0:-2])\n",
    "print('Repo is in: {}'.format(homedir))\n",
    "sys.path.insert(1, homedir)\n",
    "from src.misc import index_to_sample_df, downsample_raw_input, read_tRNAdb_info, sample_df_to_dict\n",
    "from src.read_processing import AR_merge, BC_split, Kmer_analysis, BC_analysis, UMI_trim\n",
    "from src.alignment import SWIPE_align\n",
    "from src.stats_collection import STATS_collection\n",
    "from src.plotting import TRNA_plot\n",
    "from src.transcript_mutations import TM_analysis\n",
    "\n",
    "# These are default folder names for data and raw fastq files\n",
    "# relative to the folder in which this notebook is in:\n",
    "data_dir = 'data'\n",
    "seq_dir = 'raw_fastq'\n",
    "seq_dir_noDS = seq_dir # Not downsampled\n",
    "\n",
    "# These folder names are used in subsequent processing steps\n",
    "# to dump data. Best to not change:\n",
    "AdapterRemoval_dir = 'AdapterRemoval'\n",
    "BC_dir = 'BC_split'\n",
    "UMI_dir = 'UMI_trimmed'\n",
    "align_dir = 'SWalign'\n",
    "stats_dir = 'stats_collection'\n",
    "TM_dir = 'transcript_mutations'\n",
    "plotting_dir = 'plotting'\n",
    "tRNA_database = dict()\n",
    "tRNA_database['human'] = '{}/tRNA_database/human/hg38-tRNAs.fa'.format(homedir)\n",
    "tRNA_database['mouse'] = '{}/tRNA_database/mouse/mm10-tRNAs.fa'.format(homedir)\n",
    "# Read information (length, codon etc) of tRNAs into dictionary:\n",
    "tRNA_data = read_tRNAdb_info(tRNA_database)\n",
    "SWIPE_score_mat = '{}/utils/nuc_score-matrix.txt'.format(homedir)\n",
    "SWIPE_score_mat2 = '{}/utils/nuc_score-matrix_2.txt'.format(homedir) # For masked reference sequences\n",
    "# tRNA sequencing yields many duplicated reads.\n",
    "# Adding these commonly seen sequences to a list prevents duplicated alignment:\n",
    "common_seqs = '{}/utils/common-seqs.fasta.bz2'.format(homedir)\n",
    "\n",
    "# Define minimum read length based on minimum insert size:\n",
    "MIN_INSERT_LEN = 10\n",
    "UMI_LEN = 10\n",
    "BC_MAX_LEN = 19\n",
    "MIN_READ_LEN = MIN_INSERT_LEN + UMI_LEN + BC_MAX_LEN\n",
    "print('Using minimum read length: {} (after merge)'.format(MIN_READ_LEN))\n",
    "\n",
    "# The minimum alignment score.\n",
    "# Better to set relatively low, since additional filtering can\n",
    "# be applied later.\n",
    "MIN_SCORE_ALIGN = 15\n",
    "print('Using minimum alignemnt score: {}'.format(MIN_SCORE_ALIGN))\n",
    "\n",
    "# Read index information:\n",
    "index_list_fnam = 'index_list.xlsx'\n",
    "index_df = pd.read_excel('{}/utils/{}'.format(homedir, index_list_fnam))\n",
    "\n",
    "# Read sample list:\n",
    "sample_list_fnam = 'sample_list_alignment-opti.xlsx'\n",
    "sample_df = pd.read_excel('{}/{}'.format(NBdir, sample_list_fnam))\n",
    "# Add barcode sequences:\n",
    "sample_df = index_to_sample_df(sample_df, index_df)\n",
    "# Read elementary info (replicate, barcode, species)\n",
    "# for each unique sample name into a dictionary:\n",
    "sample_dict = sample_df_to_dict(sample_df)\n",
    "# Get filenames from the sample information:\n",
    "inp_file_df = sample_df[['fastq_mate1_filename', 'fastq_mate2_filename', 'P5_index', 'P7_index', 'P5_index_seq', 'P7_index_seq']].copy().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Make a dictionary with paths used for data processing:\n",
    "dir_dict = dict(NBdir = NBdir,\n",
    "                data_dir = data_dir,\n",
    "                seq_dir = seq_dir,\n",
    "                AdapterRemoval_dir = AdapterRemoval_dir,\n",
    "                BC_dir = BC_dir,\n",
    "                UMI_dir = UMI_dir,\n",
    "                align_dir = align_dir,\n",
    "                stats_dir = stats_dir,\n",
    "                TM_dir = TM_dir,\n",
    "                plotting_dir = plotting_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment optimization grid search\n",
    "\n",
    "tRNAs are heavily modified post-transcriptionally and some of these modifications induce skipping or wrong base incorporation during the reverse transcription step.\n",
    "At the level of alignment, RT skipping will appear as gaps and wrong base incorporation will appear as mismatches, both leading to lower alignment scores.\n",
    "The non-random nature of such modifications is problematic because it can lead to non-random misannotation by flipping the alignment score in favor of a closely related, but wrong, annotation.\n",
    "\n",
    "There are three strategies to increase alignment specificity: \n",
    "1. Increase alignment length i.e. decrease RT fall-off\n",
    "2. Remove tRNA modification i.e. demodify tRNA before the RT step\n",
    "3. Adjust the alignment to accomodate RT artifacts\n",
    "\n",
    "Here, we are exploring the third option by applying \"N\" masking of positions in the reference sequences that are highly modified.\n",
    "N masking causes the masked positions to have no contribution to the alignment score.\n",
    "Therefore it is a very unspecific approach, as opposed to a position specific scoring matrix (PSSM) or a hidden Markov model (HMM) that would score the types of mismatches differently.\n",
    "The advantage of the unspecific approach is that fewer assumptions are made.\n",
    "Using a PSSM or HMM would require inferring these models on a dataset of tRNA reads, with the implicit assumption that each read is drawn randomly from the distribution of all possible reads.\n",
    "That assumption is quite obviously wrong, given that the penetrance of a tRNA modification can change as a response to a biological event.\n",
    "\n",
    "In the following, reference sequence masking is made using the mismatches observed in the alignment i.e. if the tRNA reads have a high degree of mismatches on a certain position in the reference, then this position in the reference is masked.\n",
    "The masking will alter the alignment and thus iterations can be performed using the new alignment to make a new masked reference etc.\n",
    "The masking has four tuning parameters:\n",
    "1. \"min_mut_freq\": The minimum mismatch frequency to trigger masking\n",
    "2. \"unique_anno\": Use only uniquely annotated reads for masking (True/False)\n",
    "3. \"frac_max_score\": The minimum fraction of the maximum alignment score between two reference sequences to expand the masked positions in one reference to another\n",
    "4. \"iteration\": The number of iterations performed\n",
    "\n",
    "The purpose of the grid search is to find the best set of tuning parameters and use these in future alignments.\n",
    "A first pass alignment has to be generated, then the grid search is performed and finally the best performing masked reference sequences are extracted for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling UMI trimmed sequences to maximum 2000000 reads.\n"
     ]
    }
   ],
   "source": [
    "# If restarting after shutdown,\n",
    "# no need to re-run first pass alignment:\n",
    "if True:\n",
    "    # Run AdapterRemoval:\n",
    "    AR_obj = AR_merge(dir_dict, inp_file_df, MIN_READ_LEN, overwrite_dir=True)\n",
    "    inp_file_df = AR_obj.run_parallel(n_jobs=4)\n",
    "\n",
    "    # Split files based on barcodes:\n",
    "    BCsplit_obj = BC_split(dir_dict, sample_df, inp_file_df, overwrite_dir=True)\n",
    "    sample_df, inp_file_df = BCsplit_obj.run_parallel(n_jobs=12)\n",
    "\n",
    "    # Trim UMI:\n",
    "    UMItrim_obj = UMI_trim(dir_dict, sample_df, overwrite_dir=True)\n",
    "    sample_df = UMItrim_obj.run_parallel(n_jobs=12)\n",
    "\n",
    "    # First pass alignment:\n",
    "    align_obj = SWIPE_align(dir_dict, tRNA_database, sample_df, SWIPE_score_mat, \\\n",
    "                            gap_penalty=6, extension_penalty=1, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                            common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "    sample_df = align_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "    # Collect alignment statistics:\n",
    "    stats_obj = STATS_collection(dir_dict, tRNA_data, sample_df, common_seqs=common_seqs, \\\n",
    "                                 overwrite_dir=True)\n",
    "    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "    # Write first alignment stats:\n",
    "    align_res = sample_df.loc[:, ['sample_name_unique', 'Mapping_percent', \\\n",
    "                                  'percent_single_annotation', 'percent_multiple_codons']]\n",
    "    align_res['unique_anno'] = None\n",
    "    align_res['frac_max_score'] = None\n",
    "    align_res['min_mut_freq'] = None\n",
    "    align_res['iteration'] = None\n",
    "    with open('align-opti_res.csv', 'w') as fh_res:\n",
    "        align_res.to_csv(fh_res, index=False)\n",
    "\n",
    "else:\n",
    "    # Run AdapterRemoval:\n",
    "    AR_obj = AR_merge(dir_dict, inp_file_df, MIN_READ_LEN, overwrite_dir=False)\n",
    "    inp_file_df = AR_obj.run_parallel(n_jobs=4, overwrite=False)\n",
    "\n",
    "    # Split files based on barcodes:\n",
    "    BCsplit_obj = BC_split(dir_dict, sample_df, inp_file_df, overwrite_dir=False)\n",
    "    sample_df, inp_file_df = BCsplit_obj.run_parallel(n_jobs=12, load_previous=True)\n",
    "\n",
    "    # Trim UMI:\n",
    "    UMItrim_obj = UMI_trim(dir_dict, sample_df, overwrite_dir=False)\n",
    "    sample_df = UMItrim_obj.run_parallel(n_jobs=12, load_previous=True)\n",
    "\n",
    "    # First pass alignment:\n",
    "    align_obj = SWIPE_align(dir_dict, tRNA_database, sample_df, SWIPE_score_mat, \\\n",
    "                            gap_penalty=6, extension_penalty=1, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                            common_seqs=common_seqs, overwrite_dir=False, verbose=False)\n",
    "    sample_df = align_obj.run_parallel(n_jobs=12, verbose=False, load_previous=True)\n",
    "\n",
    "    # Collect alignment statistics:\n",
    "    stats_obj = STATS_collection(dir_dict, tRNA_data, sample_df, common_seqs=common_seqs, \\\n",
    "                                 overwrite_dir=False)\n",
    "    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False, load_previous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching a grid of alignment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_dict_masked = copy.deepcopy(dir_dict)\n",
    "dir_dict_masked['align_dir'] = 'SWalign_masked'\n",
    "dir_dict_masked['stats_dir'] = 'stats_collection_masked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of variables to test:\n",
    "min_mut_freq_grid = [0.5, 0.6, 0.63, 0.65, 0.68, 0.7, 0.72, 0.75, 0.8, 0.9]\n",
    "frac_max_score_grid = [0.90, 0.95, 1]\n",
    "unique_anno_grid = [True, False]\n",
    "N_iterations = 3\n",
    "total_combi = len(min_mut_freq_grid) * len(frac_max_score_grid) * len(unique_anno_grid) * N_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combi 7 of 180\n",
      "Running combi 8 of 180\n",
      "Running combi 9 of 180\n",
      "Running combi 13 of 180\n",
      "Running combi 14 of 180\n",
      "Running combi 15 of 180\n",
      "Running combi 19 of 180\n",
      "Running combi 20 of 180\n",
      "Running combi 21 of 180\n",
      "Running combi 37 of 180\n",
      "Running combi 38 of 180\n",
      "Running combi 39 of 180\n",
      "Running combi 43 of 180\n",
      "Running combi 44 of 180\n",
      "Running combi 45 of 180\n",
      "Running combi 49 of 180\n",
      "Running combi 50 of 180\n",
      "Running combi 51 of 180\n",
      "Running combi 61 of 180\n",
      "Running combi 62 of 180\n",
      "Running combi 63 of 180\n",
      "Running combi 64 of 180\n",
      "Running combi 65 of 180\n",
      "Running combi 66 of 180\n",
      "Running combi 67 of 180\n",
      "Running combi 68 of 180\n",
      "Running combi 69 of 180\n",
      "Running combi 70 of 180\n",
      "Running combi 71 of 180\n",
      "Running combi 72 of 180\n",
      "Running combi 73 of 180\n",
      "Running combi 74 of 180\n",
      "Running combi 75 of 180\n",
      "Running combi 76 of 180\n",
      "Running combi 77 of 180\n",
      "Running combi 78 of 180\n",
      "Running combi 79 of 180\n",
      "Running combi 80 of 180\n",
      "Running combi 81 of 180\n",
      "Running combi 82 of 180\n",
      "Running combi 83 of 180\n",
      "Running combi 84 of 180\n",
      "Running combi 85 of 180\n",
      "Running combi 86 of 180\n",
      "Running combi 87 of 180\n",
      "Running combi 88 of 180\n",
      "Running combi 89 of 180\n",
      "Running combi 90 of 180\n",
      "Running combi 97 of 180\n",
      "Running combi 98 of 180\n",
      "Running combi 99 of 180\n",
      "Running combi 103 of 180\n",
      "Running combi 104 of 180\n",
      "Running combi 105 of 180\n",
      "Running combi 109 of 180\n",
      "Running combi 110 of 180\n",
      "Running combi 111 of 180\n",
      "Running combi 127 of 180\n",
      "Running combi 128 of 180\n",
      "Running combi 129 of 180\n",
      "Running combi 133 of 180\n",
      "Running combi 134 of 180\n",
      "Running combi 135 of 180\n",
      "Running combi 139 of 180\n",
      "Running combi 140 of 180\n",
      "Running combi 141 of 180\n",
      "Running combi 151 of 180\n",
      "Running combi 152 of 180\n",
      "Running combi 153 of 180\n",
      "Running combi 154 of 180\n",
      "Running combi 155 of 180\n",
      "Running combi 156 of 180\n",
      "Running combi 157 of 180\n",
      "Running combi 158 of 180\n",
      "Running combi 159 of 180\n",
      "Running combi 160 of 180\n",
      "Running combi 161 of 180\n",
      "Running combi 162 of 180\n",
      "Running combi 163 of 180\n",
      "Running combi 164 of 180\n",
      "Running combi 165 of 180\n",
      "Running combi 166 of 180\n",
      "Running combi 167 of 180\n",
      "Running combi 168 of 180\n",
      "Running combi 169 of 180\n",
      "Running combi 170 of 180\n",
      "Running combi 171 of 180\n",
      "Running combi 172 of 180\n",
      "Running combi 173 of 180\n",
      "Running combi 174 of 180\n",
      "Running combi 175 of 180\n",
      "Running combi 176 of 180\n",
      "Running combi 177 of 180\n",
      "Running combi 178 of 180\n",
      "Running combi 179 of 180\n",
      "Running combi 180 of 180\n"
     ]
    }
   ],
   "source": [
    "if True: # Switch on to do the search\n",
    "    combi_counter = 0\n",
    "    # Run all the nested combinations:\n",
    "    for unique_anno in unique_anno_grid:\n",
    "        for frac_max_score in frac_max_score_grid:\n",
    "            for min_mut_freq in min_mut_freq_grid:\n",
    "                # \"iteration\" must be the innermost nest,\n",
    "                # because the result depends on the previous run:\n",
    "                for iteration in range(1, N_iterations+1):\n",
    "                    combi_counter += 1\n",
    "                    combi_key = '{}-{}-{}-{}'.format(unique_anno, frac_max_score, min_mut_freq, float(iteration))\n",
    "                    align_res_df = pd.read_csv('align-opti_res.csv')\n",
    "                    combi_key_set = {'{}-{}-{}-{}'.format(u, f, m, i) for u, f, m, i in zip(align_res_df['unique_anno'], align_res_df['frac_max_score'], align_res_df['min_mut_freq'], align_res_df['iteration'])}\n",
    "                    if combi_key in combi_key_set:\n",
    "                        continue\n",
    "                    print('Running combi {} of {}'.format(combi_counter, total_combi))\n",
    "\n",
    "                    # At first iteration use first pass alignment,\n",
    "                    # after this use the masked alignments:\n",
    "                    if iteration == 1:\n",
    "                        dir_dict_iter = dir_dict\n",
    "                    else:\n",
    "                        dir_dict_iter = dir_dict_masked\n",
    "                    # Perform transcript mutation analysis:\n",
    "                    TM_obj = TM_analysis(dir_dict_iter, sample_df, tRNA_database, \\\n",
    "                                         common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                    TM_obj.find_muts(n_jobs=12, unique_anno=unique_anno, verbose=False)\n",
    "                    TM_obj.mask_tRNA_database(min_mut_freq=min_mut_freq, frac_max_score=frac_max_score, \\\n",
    "                                              min_pos_count=100, min_tr_count=200)\n",
    "                    tRNA_database_masked = TM_obj.write_masked_tRNA_database(out_dir='tRNA_database_masked')\n",
    "\n",
    "                    # Run alignment:\n",
    "                    align_obj = SWIPE_align(dir_dict_masked, tRNA_database_masked, sample_df, SWIPE_score_mat2, \\\n",
    "                                            gap_penalty=6, extension_penalty=3, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                                            common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                    sample_df = align_obj.run_parallel(n_jobs=5, verbose=False)\n",
    "\n",
    "                    # Collect alignment statistics:\n",
    "                    stats_obj = STATS_collection(dir_dict_masked, tRNA_data, sample_df, \\\n",
    "                                                 common_seqs=common_seqs, overwrite_dir=True)\n",
    "                    stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                    # Collect alignment stats and append to file:\n",
    "                    col_sele = ['sample_name_unique', 'Mapping_percent', \\\n",
    "                                'percent_single_annotation', 'percent_multiple_codons']\n",
    "                    align_res = sample_df.loc[:, col_sele]\n",
    "                    align_res['unique_anno'] = unique_anno\n",
    "                    align_res['frac_max_score'] = frac_max_score\n",
    "                    align_res['min_mut_freq'] = min_mut_freq\n",
    "                    align_res['iteration'] = iteration\n",
    "                    with open('align-opti_res.csv', 'a') as fh_res:\n",
    "                        align_res.to_csv(fh_res, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make final masked database from best alignment parameters\n",
    "The best parameters were found in the data processing and plotting notebook: `alignment-opti_plotting.ipynb`\n",
    "The resulting masked reference sequences folder is moved out of this projects folder and into the main repo.\n",
    "Future alignments can then use these masked references for alignment directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters:\n",
    "min_mut_freq_grid = [0.68]\n",
    "frac_max_score_grid = [0.95]\n",
    "unique_anno_grid = [False]\n",
    "N_iterations = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat of the grid search above.\n",
    "# This is done so because 3 iterations\n",
    "# have to be performed.\n",
    "combi_counter = 0\n",
    "# Run all the nested combinations:\n",
    "for unique_anno in unique_anno_grid:\n",
    "    for frac_max_score in frac_max_score_grid:\n",
    "        for min_mut_freq in min_mut_freq_grid:\n",
    "            # \"iteration\" must be the innermost nest,\n",
    "            # because the result depends on the previous run:\n",
    "            for iteration in range(1, N_iterations+1):\n",
    "                # At first iteration use first pass alignment,\n",
    "                # after this use the masked alignments:\n",
    "                if iteration == 1:\n",
    "                    dir_dict_iter = dir_dict\n",
    "                else:\n",
    "                    dir_dict_iter = dir_dict_masked\n",
    "                # Perform transcript mutation analysis:\n",
    "                TM_obj = TM_analysis(dir_dict_iter, sample_df, tRNA_database, \\\n",
    "                                     common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                TM_obj.find_muts(n_jobs=12, unique_anno=unique_anno, verbose=False)\n",
    "                TM_obj.mask_tRNA_database(min_mut_freq=min_mut_freq, frac_max_score=frac_max_score, \\\n",
    "                                          min_pos_count=100, min_tr_count=200)\n",
    "                tRNA_database_masked = TM_obj.write_masked_tRNA_database(out_dir='tRNA_database_masked')\n",
    "\n",
    "                # Run alignment:\n",
    "                align_obj = SWIPE_align(dir_dict_masked, tRNA_database_masked, sample_df, SWIPE_score_mat2, \\\n",
    "                                        gap_penalty=6, extension_penalty=3, min_score_align=MIN_SCORE_ALIGN, \\\n",
    "                                        common_seqs=common_seqs, overwrite_dir=True, verbose=False)\n",
    "                sample_df = align_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                # Collect alignment statistics:\n",
    "                stats_obj = STATS_collection(dir_dict_masked, tRNA_data, sample_df, \\\n",
    "                                             common_seqs=common_seqs, overwrite_dir=True)\n",
    "                stats_df = stats_obj.run_parallel(n_jobs=12, verbose=False)\n",
    "\n",
    "                # Collect alignment stats and append to file:\n",
    "                col_sele = ['sample_name_unique', 'Mapping_percent', \\\n",
    "                            'percent_single_annotation', 'percent_multiple_codons']\n",
    "                align_res = sample_df.loc[:, col_sele]\n",
    "                align_res['unique_anno'] = unique_anno\n",
    "                align_res['frac_max_score'] = frac_max_score\n",
    "                align_res['min_mut_freq'] = min_mut_freq\n",
    "                align_res['iteration'] = iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name_unique</th>\n",
       "      <th>Mapping_percent</th>\n",
       "      <th>percent_single_annotation</th>\n",
       "      <th>percent_multiple_codons</th>\n",
       "      <th>unique_anno</th>\n",
       "      <th>frac_max_score</th>\n",
       "      <th>min_mut_freq</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0m_1</td>\n",
       "      <td>99.270539</td>\n",
       "      <td>74.027656</td>\n",
       "      <td>4.440943</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8m_1</td>\n",
       "      <td>99.318155</td>\n",
       "      <td>74.301386</td>\n",
       "      <td>4.457906</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32m_1</td>\n",
       "      <td>98.680070</td>\n",
       "      <td>72.137738</td>\n",
       "      <td>5.911871</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1h_1</td>\n",
       "      <td>99.291669</td>\n",
       "      <td>74.222362</td>\n",
       "      <td>4.060808</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4h_1</td>\n",
       "      <td>98.823837</td>\n",
       "      <td>74.138942</td>\n",
       "      <td>4.857488</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8h_1</td>\n",
       "      <td>98.795492</td>\n",
       "      <td>74.132849</td>\n",
       "      <td>4.076637</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16h_1</td>\n",
       "      <td>99.091935</td>\n",
       "      <td>73.952814</td>\n",
       "      <td>6.724284</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40h_1</td>\n",
       "      <td>98.561703</td>\n",
       "      <td>71.608583</td>\n",
       "      <td>6.213271</td>\n",
       "      <td>False</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.68</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_name_unique  Mapping_percent  percent_single_annotation  \\\n",
       "0               0m_1        99.270539                  74.027656   \n",
       "1               8m_1        99.318155                  74.301386   \n",
       "2              32m_1        98.680070                  72.137738   \n",
       "3               1h_1        99.291669                  74.222362   \n",
       "4               4h_1        98.823837                  74.138942   \n",
       "5               8h_1        98.795492                  74.132849   \n",
       "6              16h_1        99.091935                  73.952814   \n",
       "7              40h_1        98.561703                  71.608583   \n",
       "\n",
       "   percent_multiple_codons  unique_anno  frac_max_score  min_mut_freq  \\\n",
       "0                 4.440943        False            0.95          0.68   \n",
       "1                 4.457906        False            0.95          0.68   \n",
       "2                 5.911871        False            0.95          0.68   \n",
       "3                 4.060808        False            0.95          0.68   \n",
       "4                 4.857488        False            0.95          0.68   \n",
       "5                 4.076637        False            0.95          0.68   \n",
       "6                 6.724284        False            0.95          0.68   \n",
       "7                 6.213271        False            0.95          0.68   \n",
       "\n",
       "   iteration  \n",
       "0          3  \n",
       "1          3  \n",
       "2          3  \n",
       "3          3  \n",
       "4          3  \n",
       "5          3  \n",
       "6          3  \n",
       "7          3  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mapping_percent              98.979175\n",
       "percent_single_annotation    73.565291\n",
       "percent_multiple_codons       5.092901\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_res.loc[:, ['Mapping_percent', 'percent_single_annotation', 'percent_multiple_codons']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
