{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /home/sulab/tRNA-charge-seq/2-align_reads\n"
     ]
    }
   ],
   "source": [
    "import sys, os, subprocess, copy, shutil, re, glob, bz2, json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from Bio import Seq, SeqIO, SearchIO, SeqRecord\n",
    "import pandas as pd\n",
    "\n",
    "# Navigate back to workbookDir in case of re-running a code block:\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "os.chdir(workbookDir)  # If you changed the current working dir, this will take you back to the workbook dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices(lst, element):\n",
    "    result = []\n",
    "    offset = -1\n",
    "    while True:\n",
    "        try:\n",
    "            offset = lst.index(element, offset+1)\n",
    "        except ValueError:\n",
    "            return result\n",
    "        result.append(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_fasta_count(filename):\n",
    "    '''See: https://stackoverflow.com/a/9631635'''\n",
    "    def blocks(files, size=65536):\n",
    "        while True:\n",
    "            b = files.read(size)\n",
    "            if not b: break\n",
    "            yield b\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "        return(sum(bl.count(\">\") for bl in blocks(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "1. tRNA database must be formated as a Fasta file with unique headers and no white space.\n",
    "2. Swipe output must be sorted by alignment score (this is default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Comments ########\n",
    "\n",
    "### Cluster tRNA database with 90% indentity on a per anti-codon basis\n",
    "    ### Rename centriods as a generic anti-codon name e.g. AAA_cent-1, AAA_cent-2 etc.\n",
    "### Combine centroids with tRNA database and use for SW alignment\n",
    "    ### If max_score includes a centriod then that takes takes precedence\n",
    "    ### If multiple alignments merge alphabetically\n",
    "\n",
    "# Ask Andrew Behrens to make a hg19 tRNA files with SNPs as Ns.\n",
    "# Implement \"SNP sensitive\" SW alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SCORE = 25\n",
    "\n",
    "#data_folder = 'data/pilot_exp'\n",
    "#project_folder = 'projects/pilot_exp'\n",
    "data_folder = 'data/tRNAseq_lib1'\n",
    "project_folder = 'projects/tRNAseq_lib1'\n",
    "seq_folder = 'raw_fastq'\n",
    "umi_dir = 'UMI_trimmed'\n",
    "align_dir = 'SWalign'\n",
    "score_mat = '../../../2-align_reads/nuc_score-matrix.txt'\n",
    "tRNA_database = '../../../2-align_reads/tRNA_database/hg19_mature-tRNA.fa'\n",
    "#tRNA_database = '../../../2-align_reads/tRNA_database_consensus/hg19_mature-tRNA_centroids.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for data and stats:\n",
    "os.chdir('../' + data_folder)\n",
    "stats_dir = '../../' + project_folder + '/align_reads_stats'\n",
    "try:\n",
    "    os.mkdir(stats_dir) # For stats\n",
    "except:\n",
    "    pass\n",
    "    #shutil.rmtree(stats_dir)\n",
    "    #os.mkdir(stats_dir)\n",
    "# For manipulations and final data:\n",
    "try:\n",
    "    os.mkdir(align_dir) # For data\n",
    "except:\n",
    "    pass\n",
    "    #shutil.rmtree(align_dir)\n",
    "    #os.mkdir(align_dir)\n",
    "os.chdir(align_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('../' + data_folder)\n",
    "#os.chdir(align_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_Swipe_XML(SWxml, db_id_set, MIN_SCORE):\n",
    "    query_hits = dict()\n",
    "    hit_dict = {tag: [] for tag in ['score', 'query', 'name', 'qpos', 'dpos', 'qseq', 'aseq', 'dseq']}\n",
    "    pickup = True # When True, pick up hit data and store in tmp dict (\"hit_dict\")\n",
    "    flush = False # When True, flush tmp dict into \"query_hits\"\n",
    "    high_score = -999\n",
    "    hit_dict_prev = None # For debugging\n",
    "    for event, elem in SWxml:\n",
    "        # When \"result\" tag is encountered it marks the end of the hits for a query.\n",
    "        # Flush the data picked up:\n",
    "        if elem.tag == 'result':\n",
    "            elem.clear() # clear for saving memory\n",
    "            flush = True\n",
    "        # Pick up all tags defined in \"hit_dict\":\n",
    "        elif pickup and elem.tag in hit_dict:\n",
    "            hit_dict[elem.tag].append(elem.text)\n",
    "            # If all highest alignment score(s) have been picked up,\n",
    "            # stop picking up more data:\n",
    "            if elem.tag == 'score':\n",
    "                if int(elem.text) >= high_score:\n",
    "                    high_score = int(elem.text)\n",
    "                else:\n",
    "                    pickup = False\n",
    "\n",
    "        # Flush out hit results into \"query_hits\".\n",
    "        # Only if results are stored and alignment score is above minimum:\n",
    "        if flush and len(hit_dict['score']) > 0 and high_score >= MIN_SCORE:\n",
    "            # Convert alignment score to integers:\n",
    "            hit_dict['score'] = [int(s) for s in hit_dict['score']]\n",
    "            # Find all the highest scoring hits, extract indices for selection:\n",
    "            high_score_idx = indices(hit_dict['score'], high_score)\n",
    "            # Remove all hits with alignment score lower than\n",
    "            # the maximun score:\n",
    "            for tag in hit_dict:\n",
    "                hit_dict[tag] = [hit_dict[tag][hidx] for hidx in high_score_idx]\n",
    "            # Convert qpos/dpos (query/database alignment position) string to integer tuple:\n",
    "            hit_dict['qpos'] = [tuple(map(int, qp.split(','))) for qp in hit_dict['qpos']]\n",
    "            hit_dict['dpos'] = [tuple(map(int, dp.split(','))) for dp in hit_dict['dpos']]\n",
    "            # Assert that only one query sequence has been picked up:\n",
    "            ls_query = list(set(hit_dict['query']))\n",
    "            assert(len(ls_query) == 1)\n",
    "            # Start to populate the dict entry for the query sequence:\n",
    "            query = ls_query[0]\n",
    "            query_hits[query] = {'score': high_score}\n",
    "            # The \"name\" tag is the database result.\n",
    "            # First extract the right hand side of the string,\n",
    "            # corresponding to the fasta header,\n",
    "            # then sort (if multiple hits) and merge with @:\n",
    "            hit_dict['name'] = [n.split(' ')[-1] for n in hit_dict['name']]\n",
    "            for n in hit_dict['name']: # quick assertion that name is in database\n",
    "                 assert(n in db_id_set)\n",
    "            # Extract sorting index for other data to be sorted:\n",
    "            name_idx = sorted(range(len(hit_dict['name'])), key=lambda k: hit_dict['name'][k])\n",
    "            name = '@'.join([hit_dict['name'][didx] for didx in name_idx])\n",
    "            query_hits[query]['name'] = name\n",
    "            # Add qpos/dpos:\n",
    "            query_hits[query]['qpos'] = [hit_dict['qpos'][didx] for didx in name_idx]\n",
    "            query_hits[query]['dpos'] = [hit_dict['dpos'][didx] for didx in name_idx]\n",
    "            # Add alignment strings, but only for the first hit:\n",
    "            query_hits[query]['qseq'] = hit_dict['qseq'][name_idx[0]]\n",
    "            query_hits[query]['aseq'] = hit_dict['aseq'][name_idx[0]]\n",
    "            query_hits[query]['dseq'] = hit_dict['dseq'][name_idx[0]]\n",
    "\n",
    "        # After flushing, reset the variables for new data pickup:\n",
    "        if flush:\n",
    "            hit_dict_prev = hit_dict.copy() # For debugging\n",
    "            hit_dict = {tag: [] for tag in ['score', 'query', 'name', 'qpos', 'dpos', 'qseq', 'aseq', 'dseq']}\n",
    "            flush = False\n",
    "            pickup = True\n",
    "            high_score = -999\n",
    "    return(query_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../projects/tRNAseq_lib1/align_reads_stats/alignment_stats.xlsx'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Align reads to reference ###\n",
    "\n",
    "# Swipe command template: \n",
    "swipe_cmd_tmp = 'swipe --query INPUT_FILE --db DATABASE_FILE --out OUTPUT_FILE --symtype 1 --outfmt 7 --num_descriptions 5 --num_alignments 5 --evalue 0.000000001 --num_threads 12 --strand 1 --matrix SCORE_MATRIX -G 6 -E 1'\n",
    "swipe_cmd_tmp = swipe_cmd_tmp.replace('DATABASE_FILE', tRNA_database)\n",
    "swipe_cmd_tmp = swipe_cmd_tmp.replace('SCORE_MATRIX', score_mat)\n",
    "\n",
    "# Collect some stats:\n",
    "df_stats = pd.DataFrame(columns=['Filename', 'N_reads', 'N_mapped', 'percent_single_annotation', 'percent_multiple_annotation', 'Mapping_percent'])\n",
    "\n",
    "# Read the database IDs and use them to verify alignment results:\n",
    "db_id_set = set()\n",
    "for record in SeqIO.parse(tRNA_database, \"fasta\"):\n",
    "    db_id_set.add(record.id)\n",
    "\n",
    "# Files to align:\n",
    "gz_files = glob.glob('../' + umi_dir + '/*.bz2')\n",
    "\n",
    "#for fnam in gz_files[1:2]:\n",
    "for fnam in gz_files:\n",
    "    fnam_r = fnam.split('/')[-1]\n",
    "\n",
    "\n",
    "    # Tmp, delete \n",
    "    json_res = '{}_SWalign.json.bz2'.format(fnam_r[:-10])\n",
    "    if os.path.isfile(json_res):\n",
    "        continue\n",
    "    ####\n",
    "\n",
    "\n",
    "    # Convert to fasta as required by Swipe:\n",
    "    with bz2.open(fnam, 'rt') as fh_gz:\n",
    "        SeqIO.convert(fh_gz, \"fastq\", fnam[:-10] + '.fasta', 'fasta')\n",
    "\n",
    "    # Run Swipe:\n",
    "    fnam_r = fnam.split('/')[-1]\n",
    "    swipe_cmd = swipe_cmd_tmp\n",
    "    swipe_cmd = swipe_cmd.replace('INPUT_FILE', fnam[:-10] + '.fasta')\n",
    "    swipe_outfile = '{}_SWalign'.format(fnam_r[:-10])\n",
    "    swipe_cmd = swipe_cmd.replace('OUTPUT_FILE', swipe_outfile)\n",
    "    swipe_cmd = swipe_cmd.split(' ')\n",
    "    subprocess.check_call(swipe_cmd, stdout = subprocess.DEVNULL, stderr=subprocess.DEVNULL)    \n",
    "    \n",
    "    # Add \"data\" as root for the xml file:\n",
    "    swipe_outfile_xml = swipe_outfile + '.xml'\n",
    "    xml_first_line = '<data>\\n'\n",
    "    xml_last_line = '</data>\\n'\n",
    "    with open(swipe_outfile, 'r') as from_file:\n",
    "        try:\n",
    "            os.remove(swipe_outfile_xml)\n",
    "        except:\n",
    "            pass\n",
    "        with open(swipe_outfile_xml, 'a') as to_file:\n",
    "            from_file.readline()\n",
    "            to_file.write(xml_first_line)\n",
    "            shutil.copyfileobj(from_file, to_file)\n",
    "            to_file.write(xml_last_line)\n",
    "\n",
    "    # Parse XML:\n",
    "    SWxml = ET.iterparse(swipe_outfile_xml)\n",
    "    query_hits = parse_Swipe_XML(SWxml, db_id_set, MIN_SCORE)\n",
    "    \n",
    "    # Calculate stats:\n",
    "    N_reads = fast_fasta_count(fnam[:-10]+'.fasta')\n",
    "    N_mapped = len(query_hits)\n",
    "    map_p = N_mapped / N_reads * 100\n",
    "    P_ma = sum(1 for h in query_hits.values() if '@' in h['name']) / N_mapped * 100\n",
    "    P_sa = 100 - P_ma\n",
    "    df_stats.loc[len(df_stats)] = [fnam_r, N_reads, N_mapped, P_sa, P_ma, map_p]\n",
    "    \n",
    "    # Dump query_hits as JSON:\n",
    "    SWres_fnam = '{}_SWalign.json.bz2'.format(fnam_r[:-10])\n",
    "    with bz2.open(SWres_fnam, 'wt', encoding=\"utf-8\") as fh_gz:\n",
    "         json.dump(query_hits, fh_gz)\n",
    "\n",
    "    # Remove tmp files:\n",
    "    os.remove(fnam[:-10] + '.fasta')\n",
    "    os.remove(swipe_outfile)\n",
    "    os.remove(swipe_outfile_xml)\n",
    "\n",
    "# Write stats:\n",
    "df_stats.to_excel('alignment_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(align_dir + '/alignment_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
