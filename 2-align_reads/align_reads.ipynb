{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workbookDir: /home/sulab/tRNA-charge-seq/2-align_reads\n"
     ]
    }
   ],
   "source": [
    "import sys, os, subprocess, copy, shutil, re, glob, bz2, json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from Bio import Seq, SeqIO, SearchIO, SeqRecord\n",
    "import pandas as pd\n",
    "\n",
    "# Navigate back to workbookDir in case of re-running a code block:\n",
    "if not 'workbookDir' in globals():\n",
    "    workbookDir = os.getcwd()\n",
    "print('workbookDir: ' + workbookDir)\n",
    "os.chdir(workbookDir)  # If you changed the current working dir, this will take you back to the workbook dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices(lst, element):\n",
    "    result = []\n",
    "    offset = -1\n",
    "    while True:\n",
    "        try:\n",
    "            offset = lst.index(element, offset+1)\n",
    "        except ValueError:\n",
    "            return result\n",
    "        result.append(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_fasta_count(filename):\n",
    "    '''See: https://stackoverflow.com/a/9631635'''\n",
    "    def blocks(files, size=65536):\n",
    "        while True:\n",
    "            b = files.read(size)\n",
    "            if not b: break\n",
    "            yield b\n",
    "\n",
    "    with open(filename, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
    "        return(sum(bl.count(\">\") for bl in blocks(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_fastq_count_bz(filename):\n",
    "    '''See: https://stackoverflow.com/a/9631635'''\n",
    "    def blocks(files, size=65536):\n",
    "        while True:\n",
    "            b = files.read(size)\n",
    "            if not b: break\n",
    "            yield b\n",
    "\n",
    "    with bz2.open(filename, 'rt', encoding=\"utf-8\", errors='ignore') as f:\n",
    "        return(sum(bl.count(\"@\") for bl in blocks(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "1. tRNA database must be formated as a Fasta file with unique headers and no white space.\n",
    "2. Swipe output must be sorted by alignment score (this is default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Comments ########\n",
    "\n",
    "### Cluster tRNA database with 90% indentity on a per anti-codon basis\n",
    "    ### Rename centriods as a generic anti-codon name e.g. AAA_cent-1, AAA_cent-2 etc.\n",
    "### Combine centroids with tRNA database and use for SW alignment\n",
    "    ### If max_score includes a centriod then that takes takes precedence\n",
    "    ### If multiple alignments merge alphabetically\n",
    "\n",
    "# Ask Andrew Behrens to make a hg19 tRNA files with SNPs as Ns.\n",
    "# Implement \"SNP sensitive\" SW alignment\n",
    "\n",
    "# Add alignment stats after alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SCORE = 25\n",
    "CLEAN_DIR = False  # Delete old SWalign dir\n",
    "OVERWRITE = False   # Overwrite old json files\n",
    "DRY_RUN = False     # Do dry-run, print swipe commands, nothing deleted\n",
    "SP_SET = {'mouse', 'human'} # Only run if species is in set\n",
    "\n",
    "#data_folder = 'data/pilot_exp'\n",
    "#project_folder = 'projects/pilot_exp'\n",
    "data_folder = 'data/tRNAseq_lib1'\n",
    "project_folder = 'projects/tRNAseq_lib1'\n",
    "sample_list = 'sample_list_sp.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_folder = 'raw_fastq'\n",
    "umi_dir = 'UMI_trimmed'\n",
    "align_dir = 'SWalign'\n",
    "score_mat = '../../../2-align_reads/nuc_score-matrix.txt'\n",
    "tRNA_database = dict()\n",
    "tRNA_database['human'] = '../../../2-align_reads/tRNA_database/human/hg38-tRNAs.fa'\n",
    "tRNA_database['mouse'] = '../../../2-align_reads/tRNA_database/mouse/mm10-tRNAs.fa'\n",
    "#tRNA_database = '../../../2-align_reads/tRNA_database_consensus/hg19_mature-tRNA_centroids.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample/index information:\n",
    "sample_df = pd.read_excel('../' + project_folder + '/' + sample_list)\n",
    "sp_dict = {sn: sp for sn, sp in zip(sample_df['sample_name_unique'].values, sample_df['species'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for data and stats:\n",
    "os.chdir('../' + data_folder)\n",
    "stats_dir = '../../' + project_folder + '/align_reads_stats'\n",
    "try:\n",
    "    os.mkdir(stats_dir) # For stats\n",
    "except:\n",
    "    if CLEAN_DIR and not DRY_RUN:\n",
    "        shutil.rmtree(stats_dir)\n",
    "        os.mkdir(stats_dir)\n",
    "    else:\n",
    "        pass\n",
    "# For manipulations and final data:\n",
    "try:\n",
    "    os.mkdir(align_dir) # For data\n",
    "except:\n",
    "    if CLEAN_DIR and not DRY_RUN:\n",
    "        shutil.rmtree(align_dir)\n",
    "        os.mkdir(align_dir)\n",
    "    else:\n",
    "        pass\n",
    "os.chdir(align_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_Swipe_XML(SWxml, db_id_set, MIN_SCORE):\n",
    "    query_hits = dict()\n",
    "    hit_dict = {tag: [] for tag in ['score', 'query', 'name', 'qpos', 'dpos', 'qseq', 'aseq', 'dseq']}\n",
    "    pickup = True # When True, pick up hit data and store in tmp dict (\"hit_dict\")\n",
    "    flush = False # When True, flush tmp dict into \"query_hits\"\n",
    "    high_score = -999\n",
    "    hit_dict_prev = None # For debugging\n",
    "    for event, elem in SWxml:\n",
    "        # When \"result\" tag is encountered it marks the end of the hits for a query.\n",
    "        # Flush the data picked up:\n",
    "        if elem.tag == 'result':\n",
    "            elem.clear() # clear for saving memory\n",
    "            flush = True\n",
    "        # Pick up all tags defined in \"hit_dict\":\n",
    "        elif pickup and elem.tag in hit_dict:\n",
    "            hit_dict[elem.tag].append(elem.text)\n",
    "            # If all highest alignment score(s) have been picked up,\n",
    "            # stop picking up more data:\n",
    "            if elem.tag == 'score':\n",
    "                if int(elem.text) >= high_score:\n",
    "                    high_score = int(elem.text)\n",
    "                else:\n",
    "                    pickup = False\n",
    "\n",
    "        # Flush out hit results into \"query_hits\".\n",
    "        # Only if results are stored and alignment score is above minimum:\n",
    "        if flush and len(hit_dict['score']) > 0 and high_score >= MIN_SCORE:\n",
    "            # Convert alignment score to integers:\n",
    "            hit_dict['score'] = [int(s) for s in hit_dict['score']]\n",
    "            # Find all the highest scoring hits, extract indices for selection:\n",
    "            high_score_idx = indices(hit_dict['score'], high_score)\n",
    "            # Remove all hits with alignment score lower than\n",
    "            # the maximun score:\n",
    "            for tag in hit_dict:\n",
    "                hit_dict[tag] = [hit_dict[tag][hidx] for hidx in high_score_idx]\n",
    "            # Convert qpos/dpos (query/database alignment position) string to integer tuple:\n",
    "            hit_dict['qpos'] = [tuple(map(int, qp.split(','))) for qp in hit_dict['qpos']]\n",
    "            hit_dict['dpos'] = [tuple(map(int, dp.split(','))) for dp in hit_dict['dpos']]\n",
    "            # Assert that only one query sequence has been picked up:\n",
    "            ls_query = list(set(hit_dict['query']))\n",
    "            assert(len(ls_query) == 1)\n",
    "            # Start to populate the dict entry for the query sequence:\n",
    "            query = ls_query[0]\n",
    "            query_hits[query] = {'score': high_score}\n",
    "            # The \"name\" tag is the database result.\n",
    "            # First extract the right hand side of the string,\n",
    "            # corresponding to the fasta header,\n",
    "            # then sort (if multiple hits) and merge with @:\n",
    "            hit_dict['name'] = [n.split(' ')[-1] for n in hit_dict['name']]\n",
    "            for n in hit_dict['name']: # quick assertion that name is in database\n",
    "                 assert(n in db_id_set)\n",
    "            # Extract sorting index for other data to be sorted:\n",
    "            name_idx = sorted(range(len(hit_dict['name'])), key=lambda k: hit_dict['name'][k])\n",
    "            name = '@'.join([hit_dict['name'][didx] for didx in name_idx])\n",
    "            query_hits[query]['name'] = name\n",
    "            # Add qpos/dpos:\n",
    "            query_hits[query]['qpos'] = [hit_dict['qpos'][didx] for didx in name_idx]\n",
    "            query_hits[query]['dpos'] = [hit_dict['dpos'][didx] for didx in name_idx]\n",
    "            # Add alignment strings, but only for the first hit:\n",
    "            query_hits[query]['qseq'] = hit_dict['qseq'][name_idx[0]]\n",
    "            query_hits[query]['aseq'] = hit_dict['aseq'][name_idx[0]]\n",
    "            query_hits[query]['dseq'] = hit_dict['dseq'][name_idx[0]]\n",
    "\n",
    "        # After flushing, reset the variables for new data pickup:\n",
    "        if flush:\n",
    "            hit_dict_prev = hit_dict.copy() # For debugging\n",
    "            hit_dict = {tag: [] for tag in ['score', 'query', 'name', 'qpos', 'dpos', 'qseq', 'aseq', 'dseq']}\n",
    "            flush = False\n",
    "            pickup = True\n",
    "            high_score = -999\n",
    "    return(query_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting: Li1_UMI-trimmed_SWalign.json.bz2\n",
      "Overwriting: Mu2_UMI-trimmed_SWalign.json.bz2\n",
      "Overwriting: Mu4_UMI-trimmed_SWalign.json.bz2\n",
      "Overwriting: Li3_UMI-trimmed_SWalign.json.bz2\n",
      "Overwriting: Mu3_UMI-trimmed_SWalign.json.bz2\n",
      "Overwriting: Li4_UMI-trimmed_SWalign.json.bz2\n",
      "Overwriting: Li2_UMI-trimmed_SWalign.json.bz2\n",
      "Overwriting: Mu1_UMI-trimmed_SWalign.json.bz2\n"
     ]
    }
   ],
   "source": [
    "### Align reads to reference ###\n",
    "\n",
    "# Swipe command template: \n",
    "swipe_cmd_tmp = 'swipe --query INPUT_FILE --db DATABASE_FILE --out OUTPUT_FILE --symtype 1 --outfmt 7 --num_descriptions 5 --num_alignments 5 --evalue 0.000000001 --num_threads 12 --strand 1 --matrix SCORE_MATRIX -G 6 -E 1'\n",
    "swipe_cmd_tmp = swipe_cmd_tmp.replace('SCORE_MATRIX', score_mat)\n",
    "\n",
    "# Files to align:\n",
    "gz_files = glob.glob('../' + umi_dir + '/*.bz2')\n",
    "if DRY_RUN:\n",
    "    print('.bz2 files found:')\n",
    "    for fnam in gz_files:\n",
    "        print(fnam)\n",
    "for fnam in gz_files:\n",
    "    fnam_r = fnam.split('/')[-1]\n",
    "    UMI_idx = fnam_r.index('_UMI')\n",
    "    usam_nam = fnam_r[:UMI_idx]\n",
    "    species = sp_dict[usam_nam]\n",
    "    sp_tRNA_database = tRNA_database[species]\n",
    "    if species not in SP_SET:\n",
    "        continue\n",
    "    # Skip, if results file has already been made:\n",
    "    SWres_fnam = '{}_SWalign.json.bz2'.format(fnam_r[:-10])\n",
    "    if os.path.isfile(SWres_fnam) and OVERWRITE:\n",
    "        if DRY_RUN:\n",
    "            pass\n",
    "        else:\n",
    "            print('Overwriting: {}'.format(SWres_fnam))\n",
    "            os.remove(SWres_fnam)\n",
    "    elif os.path.isfile(SWres_fnam):\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if not DRY_RUN:\n",
    "        # Convert to fasta as required by Swipe:\n",
    "        with bz2.open(fnam, 'rt') as fh_gz:\n",
    "            SeqIO.convert(fh_gz, \"fastq\", fnam[:-10] + '.fasta', 'fasta')\n",
    "    else:\n",
    "        print('Basename: {}'.format(fnam_r))\n",
    "\n",
    "    # Run Swipe:\n",
    "    swipe_cmd = swipe_cmd_tmp\n",
    "    swipe_cmd = swipe_cmd.replace('DATABASE_FILE', sp_tRNA_database)\n",
    "    swipe_cmd = swipe_cmd.replace('INPUT_FILE', fnam[:-10] + '.fasta')\n",
    "    swipe_outfile = '{}_SWalign'.format(fnam_r[:-10])\n",
    "    swipe_cmd = swipe_cmd.replace('OUTPUT_FILE', swipe_outfile)\n",
    "    swipe_cmd = swipe_cmd.split(' ')\n",
    "    if DRY_RUN:\n",
    "        print('Swipe cmd: {}'.format(' '.join(swipe_cmd)))\n",
    "    else:\n",
    "        subprocess.check_call(swipe_cmd, stdout = subprocess.DEVNULL, stderr=subprocess.DEVNULL)    \n",
    "\n",
    "    if not DRY_RUN:\n",
    "        # Add \"data\" as root for the xml file:\n",
    "        swipe_outfile_xml = swipe_outfile + '.xml'\n",
    "        xml_first_line = '<data>\\n'\n",
    "        xml_last_line = '</data>\\n'\n",
    "        with open(swipe_outfile, 'r') as from_file:\n",
    "            try:\n",
    "                os.remove(swipe_outfile_xml)\n",
    "            except:\n",
    "                pass\n",
    "            with open(swipe_outfile_xml, 'a') as to_file:\n",
    "                from_file.readline()\n",
    "                to_file.write(xml_first_line)\n",
    "                shutil.copyfileobj(from_file, to_file)\n",
    "                to_file.write(xml_last_line)\n",
    "\n",
    "        # Read the database IDs and use them to verify alignment results:\n",
    "        db_id_set = set()\n",
    "        for record in SeqIO.parse(sp_tRNA_database, \"fasta\"):\n",
    "            db_id_set.add(record.id)\n",
    "\n",
    "        # Parse XML:\n",
    "        SWxml = ET.iterparse(swipe_outfile_xml)\n",
    "        query_hits = parse_Swipe_XML(SWxml, db_id_set, MIN_SCORE)\n",
    "\n",
    "        # Calculate stats:\n",
    "        N_reads = fast_fasta_count(fnam[:-10]+'.fasta')\n",
    "        N_mapped = len(query_hits)\n",
    "        map_p = N_mapped / N_reads * 100\n",
    "        P_ma = sum(1 for h in query_hits.values() if '@' in h['name']) / N_mapped * 100\n",
    "        P_sa = 100 - P_ma\n",
    "        df_stats.loc[len(df_stats)] = [fnam_r, N_reads, N_mapped, P_sa, P_ma, map_p]\n",
    "\n",
    "        # Dump query_hits as JSON:\n",
    "        with bz2.open(SWres_fnam, 'wt', encoding=\"utf-8\") as fh_gz:\n",
    "             json.dump(query_hits, fh_gz)\n",
    "\n",
    "        # Remove tmp files:\n",
    "        os.remove(fnam[:-10] + '.fasta')\n",
    "        os.remove(swipe_outfile)\n",
    "        os.remove(swipe_outfile_xml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mu3_UMI-trimmed_SWalign.json.bz2\n",
      "HCS1_UMI-trimmed_SWalign.json.bz2\n",
      "HVP2_UMI-trimmed_SWalign.json.bz2\n",
      "L-2_UMI-trimmed_SWalign.json.bz2\n",
      "L-1+2_UMI-trimmed_SWalign.json.bz2\n",
      "HR40V1_UMI-trimmed_SWalign.json.bz2\n",
      "Mu2_UMI-trimmed_SWalign.json.bz2\n",
      "HR80V1_UMI-trimmed_SWalign.json.bz2\n",
      "HR80S1_UMI-trimmed_SWalign.json.bz2\n",
      "2U1_UMI-trimmed_SWalign.json.bz2\n",
      "HR40P2_UMI-trimmed_SWalign.json.bz2\n",
      "HR40P1_UMI-trimmed_SWalign.json.bz2\n",
      "Fix1_UMI-trimmed_SWalign.json.bz2\n",
      "HR40S1_UMI-trimmed_SWalign.json.bz2\n",
      "2U2_UMI-trimmed_SWalign.json.bz2\n",
      "HR80V2_UMI-trimmed_SWalign.json.bz2\n",
      "HVS1_UMI-trimmed_SWalign.json.bz2\n",
      "Fix2_UMI-trimmed_SWalign.json.bz2\n",
      "HR30P1_UMI-trimmed_SWalign.json.bz2\n",
      "L-1_UMI-trimmed_SWalign.json.bz2\n",
      "BVS2_UMI-trimmed_SWalign.json.bz2\n",
      "0U1_UMI-trimmed_SWalign.json.bz2\n",
      "BAV2_UMI-trimmed_SWalign.json.bz2\n",
      "HR20P1_UMI-trimmed_SWalign.json.bz2\n",
      "Mu4_UMI-trimmed_SWalign.json.bz2\n",
      "FT_UMI-trimmed_SWalign.json.bz2\n",
      "Tu2_UMI-trimmed_SWalign.json.bz2\n",
      "HR30P2_UMI-trimmed_SWalign.json.bz2\n",
      "HR20S2_UMI-trimmed_SWalign.json.bz2\n",
      "Rich-2_UMI-trimmed_SWalign.json.bz2\n",
      "BVP2_UMI-trimmed_SWalign.json.bz2\n",
      "HVP1_UMI-trimmed_SWalign.json.bz2\n",
      "U-1+2_UMI-trimmed_SWalign.json.bz2\n",
      "BVP1_UMI-trimmed_SWalign.json.bz2\n",
      "Li3_UMI-trimmed_SWalign.json.bz2\n",
      "U-1_UMI-trimmed_SWalign.json.bz2\n",
      "4U1_UMI-trimmed_SWalign.json.bz2\n",
      "U-2_UMI-trimmed_SWalign.json.bz2\n",
      "HAV1_UMI-trimmed_SWalign.json.bz2\n",
      "BVV1_UMI-trimmed_SWalign.json.bz2\n",
      "HVV2_UMI-trimmed_SWalign.json.bz2\n",
      "8U1_UMI-trimmed_SWalign.json.bz2\n",
      "0U2_UMI-trimmed_SWalign.json.bz2\n",
      "Rich-1+2_UMI-trimmed_SWalign.json.bz2\n",
      "HVV1_UMI-trimmed_SWalign.json.bz2\n",
      "HR20V1_UMI-trimmed_SWalign.json.bz2\n",
      "HR30S2_UMI-trimmed_SWalign.json.bz2\n",
      "Tu3_UMI-trimmed_SWalign.json.bz2\n",
      "Li1_UMI-trimmed_SWalign.json.bz2\n",
      "Li2_UMI-trimmed_SWalign.json.bz2\n",
      "HCS2_UMI-trimmed_SWalign.json.bz2\n",
      "BVV2_UMI-trimmed_SWalign.json.bz2\n",
      "Tu1_UMI-trimmed_SWalign.json.bz2\n",
      "HR30V2_UMI-trimmed_SWalign.json.bz2\n",
      "Mu1_UMI-trimmed_SWalign.json.bz2\n",
      "A-2_UMI-trimmed_SWalign.json.bz2\n",
      "8U2_UMI-trimmed_SWalign.json.bz2\n",
      "HR20P2_UMI-trimmed_SWalign.json.bz2\n",
      "A-1+2_UMI-trimmed_SWalign.json.bz2\n",
      "HR30S1_UMI-trimmed_SWalign.json.bz2\n",
      "HAV2_UMI-trimmed_SWalign.json.bz2\n",
      "HAS2_UMI-trimmed_SWalign.json.bz2\n",
      "HR40V2_UMI-trimmed_SWalign.json.bz2\n",
      "CyP_UMI-trimmed_SWalign.json.bz2\n",
      "HR80P2_UMI-trimmed_SWalign.json.bz2\n",
      "HR40S2_UMI-trimmed_SWalign.json.bz2\n",
      "HAS1_UMI-trimmed_SWalign.json.bz2\n",
      "BVS1_UMI-trimmed_SWalign.json.bz2\n",
      "HR80P1_UMI-trimmed_SWalign.json.bz2\n",
      "Fix3_UMI-trimmed_SWalign.json.bz2\n",
      "HCV2_UMI-trimmed_SWalign.json.bz2\n",
      "CyA_UMI-trimmed_SWalign.json.bz2\n",
      "BAV1_UMI-trimmed_SWalign.json.bz2\n",
      "Li4_UMI-trimmed_SWalign.json.bz2\n",
      "BVR1_UMI-trimmed_SWalign.json.bz2\n",
      "A-1_UMI-trimmed_SWalign.json.bz2\n",
      "HR20V2_UMI-trimmed_SWalign.json.bz2\n",
      "HR20S1_UMI-trimmed_SWalign.json.bz2\n",
      "U-NaCl_UMI-trimmed_SWalign.json.bz2\n",
      "HVS2_UMI-trimmed_SWalign.json.bz2\n",
      "4U2_UMI-trimmed_SWalign.json.bz2\n",
      "Rich-NaCl_UMI-trimmed_SWalign.json.bz2\n",
      "HCV1_UMI-trimmed_SWalign.json.bz2\n",
      "BAR1_UMI-trimmed_SWalign.json.bz2\n",
      "HR80S2_UMI-trimmed_SWalign.json.bz2\n",
      "BVR2_UMI-trimmed_SWalign.json.bz2\n",
      "A-NaCl_UMI-trimmed_SWalign.json.bz2\n",
      "L-NaCl_UMI-trimmed_SWalign.json.bz2\n",
      "BAR2_UMI-trimmed_SWalign.json.bz2\n",
      "HR30V1_UMI-trimmed_SWalign.json.bz2\n",
      "Tu4_UMI-trimmed_SWalign.json.bz2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../projects/tRNAseq_lib1/align_reads_stats/alignment_stats.xlsx'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate alignment statistics ###\n",
    "\n",
    "df_stats = pd.DataFrame(columns=['Filename', 'N_reads', 'N_mapped', 'percent_single_annotation', 'percent_multiple_annotation', 'Mapping_percent'])\n",
    "gz_files = glob.glob('*.bz2')\n",
    "for fnam in gz_files:\n",
    "    fnam_r = fnam.split('/')[-1]\n",
    "    print(fnam)\n",
    "    # Read query_hits from JSON:\n",
    "    with bz2.open(fnam, 'rt', encoding=\"utf-8\") as fh_gz:\n",
    "         query_hits = json.load(fh_gz)\n",
    "\n",
    "    # Calculate stats:\n",
    "    N_reads = fast_fastq_count_bz('../' + umi_dir + '/' + fnam[:-17] + '.fastq.bz2')\n",
    "    N_mapped = len(query_hits)\n",
    "    map_p = N_mapped / N_reads * 100\n",
    "    P_ma = sum(1 for h in query_hits.values() if '@' in h['name']) / N_mapped * 100\n",
    "    P_sa = 100 - P_ma\n",
    "    df_stats.loc[len(df_stats)] = [fnam_r, N_reads, N_mapped, P_sa, P_ma, map_p]\n",
    "\n",
    "# Write stats:\n",
    "df_stats.to_excel('alignment_stats.xlsx')\n",
    "\n",
    "os.chdir('..')\n",
    "# Move stats files to project folder:\n",
    "shutil.copy2(align_dir + '/alignment_stats.xlsx', stats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
